[toc]

# 操作系统

## 进程与线程

### 死锁

#### 死锁的必要条件

- 互斥条件：不能同时获得锁
- 请求与保持：请求锁无法获得时，不会释放已有的锁
- 不可剥夺：持有的锁在未使用完成之前，不可以被其它进程剥夺
- 循环等待：A等待B的资源，B等待A的

### 调度算法

#### FCFS：先来先服务算法

进程按照它们的请求顺序来使用CPU。

优点：实现简单、便于运行。

缺点：当同时有CPU密集型任务和IO密集型任务时，会导致设备的利用率低。有利于长作业不利于短作业，有利于CPU密集型任务但是不利于IO密集型任务。

#### SJF：短作业优先

当有多个任务到达时，选取估计运行时间最短的任务进行执行。

优点：便于实现

缺点：效率不高，忽视了作业的等待时间，会出现饥饿现象。精确的获取一个进程的运行时间是办不到的。

#### 最短剩余时间优先

每次有新的进程请求CPU时，比较当前任务需要执行时间与已经运行任务的剩余所需时间，执行需要时间短的那个任务。因此这个算法是抢占式的。

#### 时间片轮转调度

将CPU时间划分为一个一个的时间片，每个时间片执行某个任务，如果在给定的时间片内进程没有执行完，那么就剥夺CPU并让其它进行执行。

时间片设置的太短会导致CPU的利用率低，CPU花费较大比例的时间在进程的上下文切换中。而时间片设置的太短又不利于交互式程序，会导致进程的响应时间变长。

#### 优先级调度

给每个进程赋予一个优先级，允许优先级高的的进程先运行。

防止高优先级进程一直运行的方法：①进程运行一段时间后就降低它的优先级；②给每个进程分配一个最大时间片，高优先级的进程运行完这个时间片的时间后，让次优先级的进程运行。

#### 多级队列

属于最高优先级类的进程运行一个时间片，属于第二级的进程运行两个时间片，处于第三级的进程运行四个时间片，以此类推。当一个进程用完分配的时间片后，它被移到下一级。为CPU密集型进程设置较长的时间片比频繁的分给它们很短的时间片要更为高效。

#### 银行家算法 //TO-DO:

#### 周转时间 //TO-DO

#### 锁、管程、信号量

#### 生产者消费者模型

### 进程之间通信

- **管道和FIFO（命名管道）**

  管道是进程之间的一个单向数据流，一个进程写入管道的所有数据都由内核定向到另一个进程，另一个进程由此就可以从管道中读取数据。在类Unix中可以用 ”|“ 来创建管道。

  需要注意管道和重定向的区别：```command > file``` or```file < command```。

  虽然管道很简洁高效，但是它有一个问题，就是无法打开已经存在的管道，除非管道由一个共同的祖先创建。

  一个管道可以被多个进程使用，因此需要使用文件加锁机制或者IPC信号量机制进行同步。

  管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据：管道一端的进程顺序地将进程数据写入缓冲区，另一端的进程则顺序地读取数据，该缓冲区可以看做一个循环队列，读和写的位置都是自动增加的，一个数据只能被读一次，读出以后再缓冲区都不复存在了。当缓冲区读空或者写满时，有一定的规则控制相应的读进程或写进程是否进入等待队列，当空的缓冲区有新数据写入或慢的缓冲区有数据读出时，就唤醒等待队列中的进程继续读写。

  **命名管道（FIFO）**：```mkfifo \<pipe-name>```或者```mknod p \<pipe-name>```。命名管道虽然在文件系统中不用于磁盘块，但是**拥有磁盘索引节点（inode），因此使得任何进程都可以访问FIFO。**

- **信号量（System V IPC：semget()）**

  信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。**如果受保护的资源是可用的，那么信号量的值就是正数，否则就是负数。**

- **消息队列（System V IPC：msgget()）**

  消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。

- **共享内存（System V IPC：shmget()）**

  共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。**它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。**

  通过调用shmat()把一个共享内存区”附加attach“到一个进程上。调用的进程会获取这个内存区域的起始线性地址。shmat()不修改进程的页表？

  - 共享内存为什么是最快的IPC方式？

    

- **套接字：tcp/ip**

IPC是进程间通信（Interprocess Communication）的缩写，通常指允许用户态进程执行下列操作的一组机制。

- 通过<u>信号量</u>与其他进程进行同步
- 向其他进程发送消息或者从其他进程接收消息
- 和其他进程共享一段内存区。

IPC数据结构是在进程请求IPC资源（信号量、消息队列或者共享内存区）时动态创建的。每个IPC资源都是持久的，除非被进程显式的释放，否则一直驻存在内存里。

### 进程同步

#### 每CPU变量

实际上是数组结构的数组。每个cpu对应一个变量，每个cpu只能访问它自己的变量，因此不存在竞争条件。这个类似线程本地变量。每cpu变量会在主存中排列以确保每个数据结构存放在硬件告诉缓存的不同行，防止出现伪共享的问题。

#### 原子操作

由硬件提供支持，使用总线锁（开销大）、使用缓存锁（缓存一致性协议）、使用CAS操作（cmpxchg）。

- 进行零次或一次内存访问的汇编指令是原子的。
- 读之后写之前如果没有其他cpu占用内存总线，那么该操作是原子的。
- 操作码前缀是lock的汇编指令是原子的，执行时控制单元会锁住内存总线。
- 操作码前缀是rep的汇编指令不是原子的。

#### 内存屏障

- 禁止编译器进行指令重排序
- 内存屏障确保屏障之前的操作不会跨越屏障，重排序到屏障之后执行。
- 通过c的asm volatile("": : : "memory");

#### 自旋锁

- 如果获取不到锁，就忙等

#### 顺序锁

- 与读/写自旋锁类似，区别在于写锁优先级高。

#### 信号量

- 实现了加锁原语。
- 如果值大于0代表资源可用，如果等于0信号量是忙的，但没有进程在等待资源。如果小于0，则代表资源不可用且至少有一个进程在等待资源。



## 内存管理

### 端口独立编址和端口统一编址

- 端口独立编址
  - 不将显卡、声卡等编址到存储器地址空间中，而是另行独立编址。缺点是需要特殊的指令提供硬件支持。
- 端口统一编址
  - 直接将显卡、声卡、外设等的地址编址到存储器地址空间中。缺点是会占用存储器可用内存。

### 动态共享库怎么实现的？

mmap()，**硬盘文件的内容**直接映射到内存, 任何应用程序都可通过Linux的mmap()系统调用请求这种映射。内存映射是一种**方便高效的文件I/O方式**，。普通文件被映射到进程地址空间后，进程可以像访问普通内存一样对文件进行访问，不必再调用read()/write()等操作。

### mmap()实现原理

映射方法可以将任意来源的数据传输到进程的虚拟地址空间中。作为映射目标的地址空间区域，可以像普通内存那样用通常的方法访问。但任何修改都会自动传输到原数据源。

过程：

- 进程在用户空间调用mmap，在当前进程的虚拟地址空间中，寻找一段空闲的满足要求的连续的虚拟地址。
- 调用内核系统调用，实现文件物理地址到进程虚拟地址的映射。

### 无内存抽象

### 地址空间

一个进程可用于寻址的地址集合。比较难的是给每个程序一个自己独有的地址空间，使得一个程序中的某个地址28与另一个程序中的地址28不同。

#### 基址寄存器与界限寄存器

上面问题的一个简单解决方法是动态重定位，简单的把每个进程的地址空间映射到物理内存的不同部分。

### 交换技术

程序装入内存运行一段时间，之后换出到磁盘，换入别的程序接着运行。这里说的交换技术，程序需要完整且连续的装入。

### 空闲内存管理 

#### 位图bitmap

内存被划分为一定大小的单元，对应bitmap中的一位。缺点是要一次分配k个存储单元大小的内存时，需要在位图中找到连续k个连续0的单元，比较耗时。

#### 使用链表管理

每个节点包括空闲区（H）或进程（P）的指示标志（即是否已经分配）、起始地址、长度、指向下一个节点的指针。

#### 空闲内存分配算法

1）首次适配算法：沿链表搜索，直到找到一个能满足程序申请内存大小的空闲区，如果空闲区还有剩余内存则将空闲区拆开。

2）下次适配算法：与首次适配算法类似，只不过每次分配后会记录当前位置，下次分配时从当前位置开始搜索。

3）最佳适配算法：搜索整个链表，找到一块最适合的空间。

4）最差适配算法：搜素整个链表，找到最大的空间分配。

5）快速适配算法：为常用大小的空闲区维护单独的链表。

#### 伙伴算法 Buddy System

《深入理解Linux内核》：

内核应该为分配一组连续的页框建立一种高效、健壮的分配策略，以解决内存外碎片的问题。外碎片问题即：频繁地请求和释放不同大小的一组连续页框，必然导致在已分配的块中分散了很多小的空闲页框，由此带来的问题是即使有足够的空闲页框可以满足需求，但是要分配一个大块的连续页框就无法满足。



这种称为外碎片的问题，解决方案有两种，第一种，（也就是题主想到的，这说明题主想的办法是可行的，但是为什么 Linux 没有使用，就交到下文交代了）利用分页单元将一组非连续的页框映射为连续的；第二种，使用 buddy system 将外碎片转换为内碎片方便管理。

为什么 Linux 不使用第一种呢？

1. 在某些时候连续的页框是必要的。典型的例子就是 DMA 处理器分配缓冲区的内存请求。因为在一次单独的 I/O 操作中传输一个硬盘扇区的数据时，DMA 会忽略分页单元而直接访问地址总线，所以请求的缓冲区就必须在连续的页框中。
2. 即使连续页框的分配不是很重要，但是它在保存内核页表不变方面所起的作用也是不容忽视的。频繁地修改页表会导致效率下降。
3. 内存通过 4MB 的页可以访问大块连续的物理内存，减少了后援缓冲器的失效率，提高了访问内存的平均速率。

**Linux使用Buddy System来管理空闲内存，解决外碎片问题。**

Buddy System把所有空闲页框分组为11个块链表，每个块链表包含大小为1，2，4，8，16，32，64，128，256，512，1024个连续的页框。对于1024个连续页框块的最大请求为大小为4MB的连续的RAM空闲空间。每个块的第一个页框的物理地址是该块大小的整数倍。例如，大小为16个页框的块，其起始物理地址为16*2^12的倍数（2^12=4096=4KB，为一个物理页框的大小）。

Buddy System的工作原理：

假设我们现在请求一个大小为256个连续页框的块（1MB）。算法首先在256个页框大小的块链表中寻找空的，如果有空的就分配；如果没有空的就进入到大小为512个连续页框的块链表中寻找一个空块，如果有，内核就将这个空块分成大小均为256个连续页框的两部分，一部分分配给进程，另一部分加入到大小为256的块链表中；如果512个页框大小的块链表中也没有空闲的块，那么就进入到大小为1024个页框大小的块链表中寻找空闲块，如果找到了，就将这个块分成大小为256、256、512的三个块，其中一个256的分配给进程，后面两个分别加入到256和512的块链表中。如果还没有，返回错误信号。

释放的过程与上述过程相反，也是算法名字的由来，内核试图把大小为b的两个空闲块合并为一个大小为2b的连续空闲块。

伙伴算法采用页框为基本内存区，适用于大块内存的请求。不适用于小内存块的管理，因此Linux又引入了slab分配器。

#### 为什么需要分配连续的页框

- ISA总线的DMA处理器有一个严格的限制：它们只能对RAM的前16MB寻址

#### Linux物理内存地址空间划分

- ZONE_DMA的范围是0~16M，该区域的物理页面专门供I/O设备的DMA使用。之所以需要单独管理DMA的物理页面，是因为DMA使用物理地址访问内存，不经过MMU，并且需要连续的缓冲区，所以为了能够提供物理上连续的缓冲区，必须从物理地址空间专门划分一段区域用于DMA。

- ZONE_NORMAL的范围是16M~896M，该区域的物理页面是内核能够直接使用的。

- ZONE_HIGHMEM的范围是896M~结束，该区域即为高端内存，内核不能直接使用。

由于ZONE_NORMAL和内核线性空间存在直接映射关系，所以内核会将频繁使用的数据如kernel代码、GDT、IDT、PGD、mem_map数组等放在ZONE_NORMAL里。而将用户数据、页表(PT)等不常用数据放在ZONE_ HIGHMEM里，只在要访问这些数据时才建立映射关系(kmap())。比如，当内核要访问I/O设备存储空间时，就使用ioremap()将位于物理地址高端的mmio区内存映射到内核空间的vmalloc area中，在使用完之后便断开映射关系。

#### Linux虚拟地址空间划分

用户进程的代码区一般从虚拟地址空间的0x08048000开始，这是为了便于检查空指针。代码区之上便是数据区，未初始化数据区，堆区，栈区，以及参数、全局环境变量。

![](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/wKiom1NnADvC07kmAACCVbbosf0493.png)

#### Linux虚拟地址与物理地址的映射关系

![](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/wKioL1Nm_qiCvmYxAADRFZV_1jQ205.jpg)

由于开启了分页机制，内核想要访问物理地址空间的话，必须先建立映射关系，然后通过虚拟地址来访问。为了能够访问所有的物理地址空间，就要将全部物理地址空间映射到1G的内核线性空间中，这显然不可能。于是，内核将0~896M的物理地址空间一对一映射到自己的线性地址空间中，这样它便可以随时访问ZONE_DMA和ZONE_NORMAL里的物理页面；**此时内核剩下的128M线性地址空间不足以完全映射所有的ZONE_HIGHMEM，Linux采取了动态映射的方法，即按需的将ZONE_HIGHMEM里的物理页面映射到kernel space的最后128M线性地址空间里，使用完之后释放映射关系，以供其它物理页面映射。虽然这样存在效率的问题，但是内核毕竟可以正常的访问所有的物理地址空间了。**

在64位体系结构的中，ZONE_HIGHMEM永远是空的，因为这些体系结构上可用的虚拟地址空间远大于实际安装的地址空间。

#### DMA

DMA(Direct Memory Access，直接[存储器](https://baike.baidu.com/item/存储器/1583185)访问) 是所有现代[电脑](https://baike.baidu.com/item/电脑/124859)的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于[ CPU ](https://baike.baidu.com/item/ CPU /120556)的大量中断负载。否则，CPU 需要从来源把每一片段的资料复制到[暂存器](https://baike.baidu.com/item/暂存器/4308343)，然后把它们再次写回到新的地方。在这个时间中，CPU 对于其他的工作来说就无法使用。

**原理**：

DMA传输将数据从一个地址空间复制到另一个地址空间，提供在外设和存储器之间或者存储器和存储器之间的高速数据传输。当CPU初始化这个传输动作，传输动作本身是由DMA控制器来实现和完成的。DMA传输方式无需CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场过程，通过硬件为RAM和IO设备开辟一条直接传输数据的通道，使得CPU的效率大大提高。



使用DMA的好处就是它不需要CPU的干预而直接服务外设，这样CPU就可以去处理别的事务，从而提高系统的效率，对于慢速设备，如UART，其作用只是降低CPU的使用率，但对于高速设备，如硬盘，它不只是降低CPU的使用率，而且能大大提高硬件设备的吞吐量。因为对于这种设备，CPU直接供应数据的速度太低。 因CPU只能一个总线周期最多存取一次总线，而且对于ARM，它不能把内存中A地址的值直接搬到B地址。它只能先把A地址的值搬到一个寄存器，然后再从这个寄存器搬到B地址。也就是说，对于ARM，要花费两个总线周期才能将A地址的值送到B地址。而DMA就不同了，一般系统中的DMA都有突发（Burst）传输的能力，在这种模式下，DMA能一次传输几个甚至几十个字节的数据，所以使用DMA能使设备的吞吐能力大为增强。



#### 地址总线、数据总线、控制总线的区别

- 地址总线：用于内存寻址
- 数据总线：传输数据
- 控制总线：CPU对外部部件的控制通过控制总线

#### slab算法

slab是Linux操作系统的一种内存分配机制。其工作是针对一些经常分配并释放的对象，如进程描述符等，这些对象的大小一般比较小，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统，从而避免这些内碎片。slab分配器并不丢弃已分配的对象，而是释放并把它们保存在内存中。当以后又要请求新的对象时，就可以从内存直接获取而不用重复初始化。 



在内核中，经常会使用一些链表，链表中会申请许多相同结构的结构体，比如文件对象，进程对象等等，如果申请比较频繁，那么为它们建立一个内存池，内存池中都是相同结构的结构体，当想申请这种结构体时，直接从这种内存池中取一个结构体出来，是有用且速度极快的。一个物理页就可以作用这种内存池的载体，进而进行充分利用，减少了内部碎片的产生。

所以，Slab 相当于内存池思想，且是为了解决内碎片而产生的，slab的核心思想是以对象的观点管理内存。

slab分配器是基于对象进行管理的，所谓的对象就是存放一组数据结构的内存区，为便于理解可把对象看作内核中的数据结构（例如：task_struct,file_struct 等），其方法就是构造或析构函数，构造函数用于初始化数据结构所在的内存区，而析构函数收回相应的内存区。相同类型的对象归为一类，**<u>每当要申请这样一个对象时，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统，从而避免内部碎片</u>**。slab分配器并不丢弃已经分配的对象，而是释放并把它们保存在内存中。**<u>slab分配对象时，会使用最近释放的对象的内存块，因此其驻留在cpu高速缓存中的概率会大大提高。</u>**为了避免重复初始化对象，Slab分配模式并不丢弃已分配的对象，而是释放但把它们依然保留在内存中。当以后又要请求分配同一对象时，就可以从内存获取而不用进行初始化，这是在Solaris 中引入Slab的基本思想。实际上，Linux中对Slab分配模式有所改进，它对内存区的处理并不需要进行初始化或回收。出于效率的考虑，Linux并不调用对象的构造或析构函数，而是把指向这两个函数的指针都置为空。Linux中引入Slab的主要目的是为了减少对伙伴算法的调用次数。



slab 列表中的**每个 slab 都是一个连续的内存块**（一个或多个连续页，通常为一页），它们被划分成一个个对象。这些对象是从特定缓存中进行分配和释放的基本元素。**注意 slab 是 slab 分配器进行操作的最小分配单位**，因此如果需要对 slab 进行扩展，这也就是所扩展的最小值。通常来说，每个 slab 被分配为多个对象。

由于对象是从 slab 中进行分配和释放的，因此单个 slab 可以在 slab 列表之间进行移动。例如，当一个 slab 中的所有对象都被使用完时，就从 slabs_partial 列表中移动到 slabs_full 列表中。当一个 slab 完全被分配并且有对象被释放后，就从 slabs_full 列表中移动到 slabs_partial 列表中。当所有对象都被释放之后，就从 slabs_partial 列表移动到 slabs_free 列表中。

slab 分配器首先从部分空闲的slab 进行分配。如没有，则从空的slab 进行分配。如没有，则从物理连续页上分配新的slab，并把它赋给一个cache ，然后再重新slab 分配空间。

slabs_full：完全分配的 slab
slabs_partial：部分分配的 slab
slabs_free：空 slab，或者没有对象被分配

举例：

如果有一个名叫inode_cachep的struct kmem_cache节点，它存放了一些inode对象。当内核请求分配一个新的inode对象时，slab分配器就开始工作了：

- 首先要查看inode_cachep的slabs_partial链表，如果slabs_partial非空，就从中选中一个slab，返回一个指向已分配但未使用的inode结构的指针。完事之后，如果这个slab满了，就把它从slabs_partial中删除，插入到slabs_full中去，结束；
- 如果slabs_partial为空，也就是没有半满的slab，就会到slabs_empty中寻找。如果slabs_empty非空，就选中一个slab，返回一个指向已分配但未使用的inode结构的指针，然后将这个slab从slabs_empty中删除，插入到slabs_partial（或者slab_full）中去，结束；
- 如果slabs_empty也为空，那么没办法，cache内存已经不足，只能新创建一个slab了。

[slab算法](https://blog.csdn.net/qq_22238021/article/details/80214759)

### 虚拟内存

产生原因：管理软件的膨胀，需要运行的程序往往大到内存无法容纳。

原理：每个进程都有自己的地址空间，这个空间被分割成多个块，每一个块称为页或者页面。不同进程之间的可用地址互不影响（不完全不影响，比如32位linux下，所有进程3G~4G这部分虚拟地址空间是留给内核的，而且就算是OS X的Mach内核这种内核虚拟空间与进程虚拟空间独立的内核下，也是有一部分内存是保留的，因为要留出系统调用需要的接口等）。

页表：页表中存储着虚拟地址与物理地址的映射关系，因此页表必须是进程独立的，即不一样的线程是有不同的页表的。x86架构下，CPU中有专门的页表基址寄存器，其中存放了内存中页表的基址。

多级页表：采用单级页表时，需要将整个页表都装入内存，当虚拟地址空间很大时，就会产生内存的浪费，因此采用多级页表，在进程没有使用很多内存时，只有页目录和少量页表存储在内存中，这样就减少了内存的浪费。

倒排页表：在这种设计中，每个物理内存对应一个表项，而不是一个虚拟页面对应一个表项。

### 页面置换算法

当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出，为即将调入的页面腾出存储空间。如果页面已经被修改过，那么需要将其写回硬盘；如果页面未被修改，那么只需要让换入的页面覆盖它即可。虽然可以随机选取一个页面换出，但是选择以后不常使用的页面可以提升系统性能。

#### 为什么一个页面的大小是4KB

- 页面太大容易出现大的内碎片，太小的又会导致页表很大
- 效率考虑，内存和磁盘传输数据时，4KB的块大部分情况下要比4MB的块快，虽然4KB和4MB都是磁盘块大小的倍数。

当发生缺页中断时，OS需要在内存中选择一个页面将其换出内存，以便为换入的页面腾出空间。虽然可以随机的选择一个页面淘汰，但是如果选择不经常使用的页面换出的话会提升系统的性能。

#### 最优页面置换算法

该算法不可能实现，因为实现该算法需要“预测未来”。该算法的策略是换出最晚要使用的页面。

#### 最近未使用算法NRU

在大多数具有虚拟内存的系统中，系统为每个页面设置了两个状态位，一个用于标识页面是否被读取（R），另一个标识页面是否被修改（M）。按这两位的值操作系统把页面分为四类：

- 0类：既没有被访问又没有被修改      00  （RM）
- 1类：没有被访问但是被修改              01
- 2类：已被访问未被修改                      10
- 3类：已被访问已被修改                      11

其中第1类貌似不可能出现，但其实当一个第3类页面在时钟中断后R位被设置为0就会出现1类。

NRU的思想就是需要换出时，从这4类中挑选一个编号最小的换出。隐含的思想就是换出一个没有被访问但已修改的页面要比换出一个被频繁访问且“干净”的页面要好。

#### 先进先出置换算法FIFO

维护一个链表，最新进入的页面放在尾部，最老的前部。

#### 第二次机会页面置换算法

FIFO可能会把经常使用的页面置换出去，为了解决这一问题，检查最老页面的R位，如果是0说明该页面又老又没被访问，可以立即置换，如果是1就将R位清0然后置于队列尾部

#### 时钟算法

相对于第二次机会来说，时钟算法把使用的链表换成了循环链表，设置一个指针指向最老的页面（最早来的），置换时检查页面的R位，如果是0就置换，否则设置页面的R位为0，指针移动到下一个页面，如果重复直到找到一个R位为0的页面就将它置换。

#### 最近最少使用页面置换算法LRU // TO-DO: 实现LRU

置换时选择未使用时间最长的页面。

LRU理论上可以实现，但是代价很高，为了使用LRU，需要在内存中维护一个所有页面的链表，最近最多使用的在表头，最近最少使用的表尾。困难之处在于如何维护这个链表，如果维护这个链表带来的性能开销很大，那么就舍本逐末了。

第二种实现是利用硬件计数器和页表实现LRU，要求页表项能够容纳一个64位的计数，硬件计数器每次执行指令后就加一，然后保存到页表项对应的计数位中。一旦发生缺页中断，OS就扫描页表，找到值最小的页表项，将其换出。

LRU开销：

当一个页面被访问时，需要在链表中找到这个页面，时间复杂度O(n)，然后将这个节点删除，再移动到链表的头部。

第二个是特殊的硬件很少的计算机有。

#### 最不常用置换NFU

每次发生缺页中断时，由OS扫描所有页面，将所有页面的R位加到它的计数器上。这个计数器大致跟踪了每个页面被访问的频繁程度，发生缺页中断时，置换计数器值最小的页面。

NFU的问题是它从来不忘记任何事情，第一次扫描时被频繁访问的页面，在第二次扫描时，其计数器的值仍然可能很高。

#### 老化算法aging

修改NFU算法，①在将页面的R位加入计数器之前，先将计数器的值右移一位，②是将R位加到计数器的最左位而不是加到最右端的位。

#### 工作集置换算法

基于程序运行的局部性原理，将程序运行最常访问的那部分内存预装入内存，目的是大大的减少缺页中断发生的次数。

#### 工作集时钟置换算法

![页面置换算法](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/image-20220304091342124.png)

### 共享库

任何在目标文件中被调用了但是没有被定义的函数都被称为**<u>未定义外部函数</u>**。

PIC：position independent code

FPIC Generate position-independent code (PIC) suitable for use in a shared library

在编译共享库时，告诉编译器不要产生使用绝对地址的指令，而是产生使用<u>相对地址</u>的指令。

共享库实际是**<u>内存映射文</u>件**的一个特例。

### 内存映射文件

进程可以发起一个系统调用，将一个文件映射到其虚拟地址空间的一部分。

### Linux等OS Kernel为什么要区分内核空间和用户空间

**TLB表项中有标志这个映射是内核映射还是用户映射的标志，那么将内核地址空间划分在固定的虚拟地址空间上就会获得一个好处：用户进程切换时（包括从用户进程切换到内核空间时），不会导致TLB中内核的地址空间映射失效。**假设不区分用户空间与内核空间，那么发生进程切换时就需要使所有TLB失效，即使是用户进程发起一个系统调用由用户态切换到内核态时，也需要进行内核虚拟空间的重新映射，显然没有前者效率高。

tips：OS X、ios的Mach内核不像Win/Linux一样划分用户空间与内核空间，只在虚拟地址空间中保留了一小部分共享的内存空间，这样做的好处是内核可以像用户进程一样访问全部的虚拟地址空间。

在Linux内核中，896MB以上的空间是没有映射到虚拟地址4G以上的，因此内核并不能直接访问这一部分内存。

### Linux等现代OS内核为什么要跳过分段机制？

因为一些平台上是不支持分段机制的，而分段机制也是x86架构下为了兼容以前的8086寻址模式而保留的。现代的OS虽然还是分段，但每个段的基址都是0，段长度也都是一样的（虚拟地址空间大小），即“平坦模式”，这样就巧妙的“跳过”了硬件要求的分段机制。

### 既然OS已经不采用分段机制了，那为什么软件还要在编译时分段呢？

1）可以将指令和数据划分到不同的虚拟地址空间，更加方便的进行权限控制，防止程序指令被有意无意的改写。

2）我们又知道CPU中的缓存是分为指令缓存和数据缓存的，那么将应用程序按指令和数据分段理论上可以提高缓存命中率。

3）对于运行的程序的多个副本，分段的情况下，可以很方便的只在内存中保留一份指令的副本，这样可以节省空间占用。

<u>开启分段和分页后的寻址方式：</u>

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/Center.png)

保护模式下开启分段后，cs、ss等段寄存器的值就不再表示一个段号了，而是GDT中的一个索引，对应的表项中含有段起始地址、界限、属性等内容。

## 虚拟文件系统



## IO

### IO分类

- 缓存I/O又被称作标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。

  ​    读操作：操作系统检查内核的缓冲区有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中。

  ​    写操作：将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用了sync同步命令。

  ​    缓存I/O的优点：1）在一定程度上分离了内核空间和用户空间，保护系统本身的运行安全；2）可以减少读盘的次数，从而提高性能。

  ​    缓存I/O的缺点：数据在传输过程中需要在应用程序地址空间和缓存之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。

- 直接IO就是应用程序直接访问磁盘数据，而不经过内核缓冲区，这样做的目的是减少一次从内核缓冲区到用户程序缓存的数据复制。比如说数据库管理系统这类应用，它们更倾向于选择它们自己的缓存机制，因为数据库管理系统往往比操作系统更了解数据库中存放的数据，数据库管理系统可以提供一种更加有效的缓存机制来提高数据库中数据的存取性能。

  ​    直接IO的缺点：如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘加载，这种直接加载会非常缓存。通常直接IO与异步IO结合使用，会得到比较好的性能。（异步IO：当访问数据的线程发出请求之后，线程会接着去处理其他事，而不是阻塞等待）

- 内存映射IO，内存映射是指将硬盘上文件的位置与进程逻辑地址空间中一块大小相同的区域一一对应，当要访问内存中一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据在用户空间和内核空间之间的拷贝操作。当大量数据需要传输的时候，采用内存映射方式去访问文件会获得比较好的效率。

# 计算机网络

## 问题

### 5G是哪一层的协议，WIFI呢？

链路层协议。

### 浏览器同源策略

https://developer.mozilla.org/zh-CN/docs/Web/Security/Same-origin_policy

### OSI七层协议对应TCP/IP协议族

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/osi-model-detail.69f69153.png)

## 应用层

应用层协议：

![application-layer](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/application-layer.a60c27d2.png)

### HTTP常用状态码及含义

- 1xx：信息性状态码
- 2xx：成功状态码
- 3xx：重定向
- 4xx：客户端错误状态码
- 5xx：服务端错误状态码

### HTTP请求方式有哪些

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/1418466-20180810112625596-2103906128.png)

GET和POST的区别：

1）GET在浏览器回退时是无害的，而POST会再次提交请求。

2）GET请求会被浏览器主动缓存，而POST需要设置

3）GET参数只能URL编码，而POST支持多种编码方式

4）GET发送一个TCP数据包，而POST发送两个（第一次发送http header，第二次body）。

###  从浏览器地址栏输入url到显示主页的过程

1）DNS解析

![img](/Users/hq/Documents/md_image/1101406-20200424172659130-893308322.png)

2）**<u>应用层</u>**：浏览器发出http/https请求

如果是https请求的话，还需要先使用ssl协议对http报文进行加密

3）**<u>传输层</u>**：TCP/UDP封装数据

传输层协议对上层传输下来的数据进行切分和封装后转发给网络层

4）网络层：IP协议封装IP地址，获取目的MAC地址

通过IP协议将IP地址封装为IP数据报，此时会使用ARP协议（ARP是一种用以解析地址的协议，根据通信方的IP就可以反查对应的MAC地址）主机发送信息时会将包含目标IP地址信息ARP请求广播到网络上所有的主机，并接受返回消息，以此确定目标的物理地址，找到目的的MAC地址

5）链路层：

把网络层交下来的IP数据添加首部和尾部，封装为MAC帧，根据目的MAC地址建立TCP连接（三次握手）。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/1101406-20200428174815244-1440340296.png)

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/1101406-20200428174728296-868616453.png)

### SSL协议

对称加密：加密和解密使用的密钥相同

非对称加密：加密和解密使用的密钥不同，**公钥加密的密文只能用私钥解密，私钥加密的密文只能用公钥解密。**

非对称加密原理：**将a和b相乘得出c是很容易的，而只给出c去算出a和b却是很难的。即对一个大数进行分解极难。**

数字证书是由权威机构——CA机构统一来进行发行，我们绝对信任这个机构，至于CA机构的安全性…反正99.99%之下都是安全的。为了防止中间有人对证书内容进行更改，有了一个**数字签名**的概念，所谓的数字签名就是把数字证书上所有的内容做一个Hash操作，得到一个固定长度然后再传给鲍勃。然而如果别人截取了这个证书然后更改内容，同时生成了新的Hash值那怎么办？处于这个考虑，**CA机构在颁发这个证书的时候会用自己的私钥将Hash值加密，从而防止了数字证书被篡改。**

**也就是说，数字证书的作用是证明某个公钥确实是由某个单位发布的。**

数字证书的使用步骤： 

- **第一步：**首先，当爱丽丝开启一个新的浏览器第一次去访问鲍勃的时候，会先让爱丽丝安装一个**数字证书**，这个数字证书里包含的主要信息就是CA机构的公钥。
- **第二步：**鲍勃发送来了CA机构颁发给自己的数字证书，爱丽丝通过第一步中已经得到的公钥解密CA用私钥加密的Hash-a(**这个过程就是非对称加密**)，然后再用传递过来的HASH算法生成一个Hash-b，如果Hash-a === Hash-b就说明认证通过，确实是鲍勃发过来的。

SSL协议握手过程：

1. **第一步**：爱丽丝给出支持SSL协议版本号，一个客户端**随机数**(Client random，请注意这是第一个随机数)，客户端支持的加密方法等信息；
2. **第二步：**鲍勃收到信息后，确认双方使用的加密方法，并返回**数字证书**，一个服务器生成的**随机数**(Server random，注意这是第二个随机数)等信息；
3. **第三步：**爱丽丝确认数字证书的有效性，然后生成一个新的**随机数**(Premaster secret)，然后使用数字证书中的公钥，加密这个随机数，发给鲍勃。
4. **第四步：**鲍勃使用自己的私钥，获取爱丽丝发来的**随机数**(即Premaster secret)；(第三、四步就是非对称加密的过程了)
5. **第五步：**爱丽丝和鲍勃通过约定的加密方法(通常是[AES算法](https://zh.wikipedia.org/wiki/高级加密标准))，使用前面三个随机数，生成**对话密钥**，用来加密接下来的通信内容；

单向散列加密算法：MD5、SHA-1、SHA-256

常见对称加密算法：DES、3DES、AES

常见非对称加密算法：RSA、ECC

### http1.0/1.1/2.0区别

**HTTP/1.0**默认是短连接，可以强制开启，HTTP/1.1默认长连接，HTTP/2.0采用**多路复用**

从http1.1开始默认都开启了Keep-Alive，保持连接特性，简单地说，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。

http2.0开始支持完全多路复用，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应。

http：在HTTP中响应体的**Connection**字段指定为keep-alive即可

### 为什么说http是无状态的？和Connection:keep-alive有什么区别？Cookie、Session？

无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。从另一方面讲，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系。

然而，随着时间的推移，人们发现静态的HTML着实无聊而乏味，增加动态生成的内容才会令Web应用程序变得更加有用。于是乎，HTML的语法在不断膨胀，其中最重要的是增加了表单（Form）；客户端也增加了诸如脚本处理、DOM处理等功能；对于服务器，则相应的出现了CGI（Common Gateway Interface）以处理包含表单提交在内的动态请求。

在这种客户端与服务器进行动态交互的Web应用程序出现之后，HTTP无状态的特性严重阻碍了这些交互式应用程序的实现，毕竟交互是需要承前启后的，简单的购物车程序也要知道用户到底在之前选择了什么商品。于是，两种用于保持HTTP状态的技术就应运而生了，一个是Cookie，而另一个则是Session。

**Cookie是客户端的存储空间，由浏览器来维持**。**具体来说cookie机制采用的是在客户端保持状态的方案，而session机制采用的是在服务器端保持状态的方案**。同时我们也看到，由于才服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的，但实际上还有其他选择，比如说重写URL和隐藏表单域。

**简单的说就是cookie和session起到保存客户端状态的作用，但是它们并没有改变http协议本身这种无状态的性质，可以理解为在应用上做了状态保留。**

[ref](https://www.cnblogs.com/ymkfnuiwgij/p/6978526.html)

### token

Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。

使用Token的目的：Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮。

### http长连接和短连接的区别

HTTP分为长连接和短连接，**本质上说的是TCP的长短连接**。TCP连接是一个双向的通道，它是可以保持一段时间不关闭的，因此TCP连接才具有真正的长连接和短连接这一说法。TCP长连接可以复用一个TCP连接，来发起多次的HTTP请求，这样就可以减少资源消耗，比如一次请求HTML，如果是短连接的话，可能还需要请求后续的JS/CSS。**如何设置长连接？**

通过在头部（请求和响应头）设置**Connection**字段指定为keep-alive，HTTP/1.0协议支持，但是是默认关闭的，从HTTP/1.1以后，连接默认都是长连接。

### tcp长连接和短连接的区别

是不是复用连接。

### DNS请求过程

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/dns_query.png)

## 运输层

运输层协议：

![transport-layer](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/transport-layer.3d064ed5.png)

### TCP连接建立过程

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/tcp_init.png)

### 三次握手时，如果客户端发送的ack丢失了，会发生什么？

服务端会定时重传syn+ack报文，总共发送一个设置的次数，如果重发指定次数之后，仍然未收到 client 的ACK应答，那么一段时间后，Server自动关闭这个连接。

客户端此时认为连接已经建立了，就会正常发送数据包了，这时候服务端会响应RST包，让客户端知道这边未接收到ACK。

### TCP握手为什么是三次，不能是两次、四次？

两次肯定是不行的，第一次握手，服务端确认了客户端是可以正常发送数据的，第二次握手客户端确认了服务端是可以接收数据和发送数据的，显然，这个时候服务端还不能确认客户端是否可以正常接收数据，因此要再进行一次握手。

握手四次可以，但是没有必要，前三次明明已经可以确认双方的发送接收能力了，为什么还要多进行一次浪费性能呢？

### TCP连接释放的过程

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/tcp_release.png)

Time-WAIT：1）防止接收端没有接收到最后的ACK，2）确保网络中不再有还未到达发送端的异常数据包

TCP 下的 IP 层协议是无法保证包传输的先后顺序的。如果双方挥手之后，一个网络四元组（src/dst ip/port）被回收，而此时网络中还有一个迟到的数据包没有被 B 接收，A 应用程序又立刻使用了同样的四元组再创建了一个新的连接后，这个迟到的数据包才到达 B，那么这个数据包就会让 B 以为是 A 刚发过来的。

### time_wait的作用

time_wait产生的原因：首先调用close()发起主动关闭的一方，在发送最后一个ACK之后会进入time_wait的状态，也就说该发送方会保持2MSL时间之后才会回到初始状态。MSL值得是数据包在网络中的最大生存时间。产生这种结果使得这个TCP连接在2MSL连接等待期间，定义这个连接的四元组（客户端IP地址和端口，服务端IP地址和端口号）不能被使用。

作用：

- 确保TCP连接的可靠释放。假设主动关闭的一方发送的最后一个确认连接关闭的ACK报文在网络中丢失了，由于TCP协议的重传机制，被动关闭的一方会重发FIN报文，因此主动关闭的一方必须要维护原有的TCP连接。在经过2MSL的时间后，才可以释放这个连接。

- 为了使旧的数据包在网络中过期消失。假设不存在这个状态，再假设当前有一条TCP连接：(local_ip, local_port, remote_ip,remote_port)，因某些原因，我们先关闭，接着很快以相同的四元组建立一条新连接。TCP协议栈是无法区分这一条新建立的连接和上一个连接是否是同一个连接。因此网络中可能还有某个上一次连接的TCP数据包，这个时候如果数据传输给了主动关闭的一方，它同样也无法区分是否是上一个连接的包，这就会导致出现很诡异的错误。

  这样就可能发生这样的情况：前一条TCP连接由local peer发送的数据到达remote peer后，会被该remot peer的TCP传输层当做当前TCP连接的正常数据接收并向上传递至应用层（而事实上，在我们假设的场景下，这些旧数据到达remote peer前，旧连接已断开且一条由相同四元组构成的新TCP连接已建立，因此，这些旧数据是不应该被向上传递至应用层的），从而引起数据错乱进而导致各种无法预知的诡异现象。作为一种可靠的传输协议，TCP必须在协议层面考虑并避免这种情况的发生，这正是TIME_WAIT状态存在的第2个原因。

### TCP释放连接时，如果主动发起一方的ACK丢失了怎么办

这个也是为什么设置time_wait状态的原因，如果主动发起一方的ACK丢失了，被动关闭的一方会重传FIN包。

### TCP挥手为什么是四次？

四次挥手是因为客户端与服务端并一定是完全同步的，客户端发送完所有数据后，服务端并不一定就发送完所有数据了。前两次挥手后，服务端和客户端都知道客户端没有新数据发送了，可以关闭连接了。此时服务端不一定发送完数据。服务器也发送完数据后，跟客户端再进行两次握手，以确保服务端和客户端都知道服务端也发送完成了。最终，TCP连接断开。

###  TCP和UDP的区别

1）TCP是面向连接的，而UDP是无连接的

2）TCP提供可靠的服务，而UDP不提供

3）TCP面向字节流，而UDP面向报文。就是说TCP发送和接收都是面向字节流的，上层不管发送给TCP协议任何数据，TCP都将这些数据当成字节流来处理。TCP会将这些数据划分或整合。比如说上层传递的数据很多的话，TCP会选择把这些数据切分，而如果很少的话，TCP会把数据先放在缓存里，等凑够一定量的数据再发送。接受同样。而UDP则不同了，UDP会将上层协议发送的报文整个的处理，既不会切分也不会整合，而是交给下层的IP协议去做。并且接收数据时UDP也是按照报文来接收的。

4）TCP数据传输慢，UDP数据传输快

5）TCP有拥塞控制，而UDP没有

### TCP滑动窗口，TCP为什么需要滑动窗口

TCP选择使用滑动窗口和TCP协议需要提供可靠的数据传输的特性有关：TCP发送数据后是要等对方确认才能发送下一个数据的。这样的方式的话，会使TCP的效率比较低。为了解决这个问题，TCP引入了滑动窗口的概念。

滑动窗口的值表示可以立即发送而无需等待确认的数据量。它告诉对方本端的TCP接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度，从而达到**流量控制**的目的。

TCP滑动窗口分为发送窗口和接收窗口。**发送端的滑动窗口**包含四大部分：已发送且接收到ACK、已发送未接收到ACK、可发送但未发送、不可发送

接收端的滑动窗口包含三个部分。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/1232796-20171016103123177-958324725.png)

### TCP拥塞控制

TCP拥塞控制包含：**慢启动算法、拥塞避免、拥塞发生、快速恢复算法**几种算法。

发送方维护一个**拥塞窗口cwnd（congestion window）**的变量，用来估算在一段时间内这条链路可以承载和运输的数据的数量。它大小代表着网络的拥塞程度，并且是动态变化的，但是为了达到最大的传输效率，我们该如何知道这条链路的运送效率是多少呢？

一个比较简单的方法就是不断增加传输的水量，直到水管快要爆裂为止（对应到网络上就是发生丢包），用 TCP的描述就是：“只要网络中没有出现拥塞，拥塞窗口的值就可以再增大一些，以便把更多的数据包发送出去，但只要网络出现拥塞，拥塞窗口的值就应该减小一些，以减少注入到网络中的数据包数。”

**<u>总结</u>**：

刚开始使用慢启动算法，避免一上来就发送大量的数据，可能导致网络的拥塞（积少成多），但是慢启动算法下的拥塞窗口增速是很快的，呈指数级（整体上看），所以一旦出现丢包，就认为此时的拥塞窗口有可能达到网络的负载极限了，因此不能再呈指数增长了，而是改成一点一点增长。而当出现网络拥塞丢包时，会有两种情况，一种是RTO超时重传，另一种是快速重传，这两种的区别要看**<u>丢包后</u>**发送端是否收到接收端的三个连续的ACK报文，如果发送端接收到了，那么就重发丢掉的包，如果接收到了丢包的ACK报文，那么就认为网络负载也不是那么糟糕，拥塞窗口只会减少比较小的一个值，然后继续执行拥塞避免算法；而如果等到了RTO计时器超时后，也没有发生前面说的情况，那么这个时候就认为网络确实出现拥塞了，这个时候就需要执行网络拥塞算法，直接一下回到解放前，拥塞窗口重设，重新执行慢启动算法。

整体来说，TCP拥塞控制会动态的调整拥塞窗口的窗口的值和sshthresh的值，尽量让发送速度保持在网络可承载的最大值上又不会导致网络拥塞。

#### 慢启动算法

慢启动算法，表面意思就是，别急慢慢来。它表示TCP建立连接完成后，一开始不要发送大量的数据，而是先探测一下网络的拥塞程度。由小到大逐渐增加拥塞窗口的大小，如果没有出现丢包，**每收到一个ACK，就将拥塞窗口cwnd大小就加1（单位是MSS）**。**每轮次**发送窗口增加一倍，呈指数增长，如果出现丢包，拥塞窗口就减半，进入拥塞避免阶段。

#### 拥塞避免算法

一般来说，慢启动阀值ssthresh是65535字节，cwnd到达**慢启动阀值**后每收到一个ACK时，cwnd = cwnd + 1/cwnd当每过一个RTT时，cwnd = cwnd + 1。显然这是一个线性上升的算法，避免过快导致网络拥塞问题。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/48540923dd54564e44e7bebec603758ad3584f99.png)

#### 拥塞发生

当网络拥塞发生**丢包**时，会有两种情况：

1）RTO超时重传2）快速重传

如果是发生了**RTO超时重传**，就会使用拥塞发生算法

慢启动阀值sshthresh = cwnd /2

cwnd 重置为 1进入新的慢启动过程

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/f603918fa0ec08fac95e13002333d46554fbda25.png)

这真的是**辛辛苦苦几十年，一朝回到解放前**。其实还有更好的处理方式，就是**快速重传**。发送方收到3个连续重复的ACK时，就会快速地重传，不必等待**RTO超时**再重传。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/d01373f082025aaf68758d458e30426c014f1adb.png)

慢启动阀值ssthresh 和 cwnd 变化如下：

拥塞窗口大小 cwnd = cwnd/2，慢启动阀值 ssthresh = cwnd，进入快速恢复算法

#### 快速恢复算法

快速重传和快速恢复算法一般同时使用。快速恢复算法认为，还有3个重复ACK收到，说明网络也没那么糟糕，所以没有必要像RTO超时那么强烈。

正如前面所说，进入快速恢复之前，cwnd 和 sshthresh已被更新：

cwnd = cwnd /2- sshthresh = cwnd然后，真正的快速算法如下

- cwnd = sshthresh + 3

- 重传重复的那几个ACK（即丢失的那几个数据包）
- 如果再收到重复的 ACK，那么 cwnd = cwnd +1
- 如果收到新数据的 ACK 后, cwnd = sshthresh。因为收到新数据的 ACK，表明恢复过程已经结束，可以再次进入了拥塞避免的算法了。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/0e2442a7d933c8959660e96ba7ce9af8830200ed.png)

### TCP的重传机制

重传包括**超时重传、快速重传、带选择确认的重传（SACK）、重复SACK四种**。

#### 超时重传

超时重传，是TCP协议保证数据可靠性的另一个重要机制，其原理是在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的ACK报文，那么就重新发送数据，直到发送成功为止。

这个一定时间内，一般是多少比较合理呢？来看下什么叫**RTT（Round-Trip Time，往返时间）**。

RTT就是数据完全发送完，到收到确认信号的时间，即数据包的一次往返时间。超时重传时间，就是RTO（Retransmission Timeout)。

#### 快速重传

其实可以使用**快速重传**，来解决超时重发的时间等待问题。它不以时间驱动，而是以数据驱动。它是基于接收端的反馈信息来引发重传的。快速重传流程如下：

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/38dbb6fd5266d016a3c77956eff63d0f37fa35dc.png)

#### 带选择确认的重传（SACK）

为了解决：**应该重传多少个包**的问题? TCP提供了**带选择确认的重传**（即SACK，Selective Acknowledgment）。

**SACK机制**就是，在快速重传的基础上，接收方返回最近收到报文段的序列号范围，这样发送方就知道接收方哪些数据包是没收到的。这样就很清楚应该重传哪些数据包啦。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/3b292df5e0fe9925df0c8e034f75b7d78cb17143-20220304214333954.png)

SACK机制如上图中，发送方收到了三次同样的ACK=30的确认报文，于是就会触发快速重发机制，通过SACK信息发现只有30~39这段数据丢失，于是重发时，就只选择了这个30~39的TCP报文段进行重发。

#### 重复SACK（D-SACK）

★D-SACK，英文是Duplicate SACK，是在SACK的基础上做了一些扩展，主要用来告诉发送方，有哪些数据包，自己重复接受了。DSACK的目的是帮助发送方判断，是否发生了包失序、ACK丢失、包重复或伪重传。让TCP可以更好的做网络流控。来看个图吧：

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/a8014c086e061d9509e9c22f0f29e3d963d9ca64.png)



### TCP协议如何来保证传输的可靠性

TCP提供一种面向连接的、可靠的字节流服务。其中，面向连接意味着两个使用TCP的应用（通常是一个客户和一个服务器）在彼此交换数据之前必须先建立一个TCP连接。在一个TCP连接中，仅有两方进行彼此通信；而字节流服务意味着两个应用程序通过TCP链接交换8bit字节构成的字节流，TCP不在字节流中插入记录标识符。

**对于可靠性，TCP通过以下方式进行保证：**

- 数据包校验：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不给出响应，这时TCP发送数据端超时后会重发数据；
- 对失序数据包重排序：既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。TCP将对失序数据进行重新排序，然后才交给应用层；
- 丢弃重复数据：对于重复数据，能够丢弃重复数据；
- 应答机制：当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒；
- 超时重发：当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段；
- 流量控制：TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。TCP使用的流量控制协议是可变大小的滑动窗口协议。

### Go-Back-N

停止和等待协议是用于流控制机制的协议。在此协议中，发送方一次发送一个帧，并等待接收方的确认。确认后，发送方将另一个帧发送给接收方。如果未收到确认，则重新传输帧/数据包。

GoBackN还是流控制机制的协议。在此协议中，发送方一次发送n帧，并等待通信确认。如果未收到确认，则重新发送整个帧。



### socket

客户端和服务器进行通信前，双方都需要创建一个socket。

- 服务端首先调用socket()函数创建socket实例。接着调用bind给socket实例绑定ip和端口号。绑定完之后调用listen()监听。
- 服务端调用accept()来从内核获取一个与客户端的连接，如果没有，那么accept()调用会阻塞。

现在服务端等待客户端连接。

客户端：

- 客户端创建socket后，调用connect()来发起连接请求，需要指明ip和端口号。接下来就是TCP三次握手的过程。
- 在tcp连接的过程中，内核为每个socket维护两个队列
  - 半连接队列
  - 全连接队列
- 当socket对应的tcp的全连接队列不全为空时，内核会从其中拿出一条tcp连接，从accept()调用返回。
- 需要注意的是，监听和用来传输数据的socket是两个
  - 一个叫做监听socket
  - 一个叫做已连接socket
- 至此，客户端与服务端建立连接了

上面这种方式只能一对一的通信。

#### 多进程/多线程

在accept返回后，会fork()一个子进程来处理通信，而监听socket的父进程仍然负责监听。

多线程思想类似。

问题是每来一个新的TCP连接，那么就需要新建一个进程/线程。也就是说一个已连接socket需要分配一个进程/线程来进行处理。

#### IO多路复用

让一个进程来管理多个已连接socket

#### select/poll

poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。

#### epoll

- epoll使用红黑树来跟踪所有socket的文件描述符，增删查改时间复杂度均在O(logN)。

### TCP如何保证可靠传输

![图片](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/tcp_realiable.png)

- 重传机制，序列号与应答。
- 滑动窗口
- 流量控制
- 拥塞控制

**重传机制**：

- 超时重传：为每个数据包设定一个计时器，如果到了指定时间还没有收到确认，那么就重发。超时时间根据RTO设置。
- 快速重传：发送端连续收到3个连续同样的ACK，就会重传数据包，这个方法和上面的方法需要一个包一个确认。但是问题是发送端会丢失某些信息。比如发送端收到了三个连续的ack 2报文，说明在发送1之后，至少又发送了4个报文，但是我们是没法指定这三个连续的ACK2到底是来自于哪个已发送的数据报的响应。
- SACK选择确认：在sack字段上返回丢失了哪些包，然后让发送端重传。SACK重传的话，ACK的值会小于SACK的范围。
- D-SACK：告诉发送方哪些数据包被重复接受了。ACK的值是大于SACK表示的范围的，表示我已经接收到了所有的了，但是你又给我发送了某一些我已经接受到的数据。

**滑动窗口：**

- 不加滑动窗口的话，传输太慢了
- 窗口大小就是指**无需等待确认应答，而可以继续发送数据的最大值**。
- 累积确认，对于丢失的确认，可以先不管，等下一次发送确认时，发送方就知道自己其实已经接收到了之前所有的数据。

**流量控制：**

- 流量控制就是让发送发送速率不要过快,让接收方来得及接收。利用滑动窗口机制就可以实施流量控制。

**拥塞控制：**

- 慢启动
- 拥塞避免
- 拥塞发生
- 快恢复

## 网际层

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/nerwork-layer.688b8282.png)

### IP地址分类

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/1440532-20180912093201807-306001370.png)

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/1440532-20180912093223517-341420925.png)

### 无分类地址CIDR

无分类地址CIDR是用来解决传统的分类IP地址分的不够细致的问题的。

### NAT

NAT不仅能解决IP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。

NAT可以将公网IP转换成私有IP，分为静态NAT、动态NAT、网络地址端口转换NAPT。

静态NAT维护公私网IP映射表，是一对一的关系，是一成不变的。

动态NAT同样维护公私网IP映射，区别是映射关系会变。

网络地址端口转换：映射时加上端口号，就可以将多个私网IP映射到同一个公网IP上

## 网络接口层（链路层+物理层）

5G、Wifi都属于链路层的协议。

![network-interface-layer](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/network-interface-layer.c1c4efcc.png)

# 数据库

## 概念

### 事务四大特性

1）原子性

2）隔离性

3）一致性

4）持久性

### 数据库隔离级别

1）串行化：最高级别的隔离，相当于是单线程了，不存在并发，因此也不存在**脏读、不可重复读、幻读**的问题。

2）可重复读：MySQL默认隔离级别。一个事务在执行过程中，可以访问其他事务成功提交的新插入的数据，但不可以访问成功修改的数据。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。此隔离级别可有效防止**不可重复读和脏读。**

3）读已提交：一个事务在执行过程中，既可以访问其他事务成功提交的新插入的数据，又可以访问成功修改的数据。读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。此隔离级别可有效防止**脏读**。

4）读未提交：最低隔离级别。一个事务在执行过程中，既可以访问其他事务未提交的新插入的数据，又可以访问未提交的修改数据。如果一个事务已经开始写数据，则另外一个事务不允许同时进行写操作，但允许其他事务读此行数据。此隔离级别可防止**丢失更新**。

**<u>事务的并发问题：</u>**

1）脏读：一个事务读取到了被另一个事务修改，但是还未提交的数据。

2）不可重复读：在一次事务中，在不同的时间查询某一个数据，发现该数据已经被修改了，可能是被更新了，也可能是被删除了

3）幻读：指一个事务执行两次查询，但第二次查询的结果包含了第一次查询中未出现的数据。

4）丢失更新：指两个事务同时更新一行数据，后提交（或撤销）的事务将之前事务提交的数据覆盖了。

### MVCC

多版本并发控制技术。原理是，通过数据行的多个版本管理来实现数据库的并发控制，简单来说就是保存数据的历史版本。可以通过比较版本号决定数据是否显示出来。读取数据的时候不需要加锁可以保证事务的隔离效果。

MVCC 可以解决什么问题？

- 读写之间阻塞的问题，通过 MVCC 可以让读写互相不阻塞，读不相互阻塞，写不阻塞读，这样可以提升数据并发处理能力。
- 降低了死锁的概率，这个是因为 MVCC 采用了乐观锁的方式，读取数据时，不需要加锁，写操作，只需要锁定必要的行。
- 解决了一致性读的问题，当我们朝向某个数据库在时间点的快照是，只能看到这个时间点之前事务提交更新的结果，不能看到时间点之后事务提交的更新结果。

InnoDB的MVCC如何实现？

[知乎-MVCC 原理](https://zhuanlan.zhihu.com/p/147372839)

[正确的理解MySQL的MVCC及实现原理](https://www.cnblogs.com/xuwc/p/13873611.html)

### B+树索引和Hash索引

数据有序,范围查询。

hash索引，等值查询效率高，不能排序,不能进行范围查询

### 聚集索引和非聚集索引

### 索引的优缺点

索引最大的好处是提高查询速度， 缺点是更新数据时效率低，因为要同时更新索引 对数据进行频繁查询进建立索引，如果要频繁更改数据不建议使用索引。

### 索引为什么用B+树实现而不是红黑树/B树

### long_query怎么解决

设置参数，开启慢日志功能，得到耗时超过一定时间的sql

### 有哪些锁,select时怎么加排它锁

| 锁     | 概念                                                  |
| ------ | ----------------------------------------------------- |
| 乐观锁 | MVCC实现                                              |
| 共享锁 | 共享锁，多个事务，只能读不能写，加 lock in share mode |
| 排它锁 | 一个事务，只能写，for update                          |
| 行锁   | 作用于数据行                                          |
| 表锁   | 作用于数据表                                          |

### 内连接、外连接、自连接

- 内连接：查询两张表交集的部分，inner join
- 左外连接：包含左表的所有数据，和左表右表共有的数据，不存在的列置null
- 右外连接：包含右表的所有数据。。。。。
- 自连接：

### 关系型数据库和非关系型数据库的区别

- 关系型数据库
  - 数据存储结构一般是二维表结构
  - 关系型数据库强调的是数据的ACID强一致性。
- 非关系型数据库
  - 数据存储结构一般是KV结构
  - 强调数据的最终一致性，因此可能读到中间状态

## SQL

### SQL基础

#### 外键

**外键删除时行为：**

![image-20220319154915116](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/foreign_key_delete_action.png)

### DDL、DML、DQL、DCL

- DQL：Data Query Language         select
- DML：**Data Manipulation Language** insert update delete
- DDL：数据定义语言

### 聚集函数（count、min、max、avg、sum）

```sql
SELECT 聚合函数(字段) FROM 
```

null值不参与聚集函数计算。

### 分组查询（GROUP BY）

```sql
SELECT 字段列表 FROM table_name [WHERE cond1] GROUP BY col_name [HAVING cond2]
```

- WHERE会在分组之前执行，不满足WHERE条件的记录，不参与分组；而HAVING是分组之后对结果进行过滤。
- 判断条件不同，WHERE不能对聚合函数进行判断，而HAVING可以
- 执行顺序：WHERE > 聚集函数 > HAVING

- 分组以后一般查询聚集函数或者分组的字段



### 常用SQL

**查看表结构：**

```sql
DESC table_name;
```

### 查询平均成绩最高的学生

1. 先查出平均成绩最高的

```sql
SELECT avg(score) as avg
FROM sc
GROUP BY sc.sno
ORDER BY avg DESC
LIMIT 1;
```

2. 查询处于最高的

```sql
SELECT student.name, avg(sc.score)
FROM student, sc
WHERE student.sno = sc.sno
GROUP BY student.sno
HAVING avg(sc.score) in {

};
```



## MySQL

### InnoDB

**特点：**

- 支持事务
- 支持行级锁
- 支持外键

**逻辑存储结构：**

- TableSpace：表空间
- Segment：     段
- Extend：        区
- Page：            页
- Row：             行

![image-20220318233150883](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_logic_space.png)

### 索引

![image-20220318233744173](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_indexes.png)



![image-20220318234008307](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_support_index.png)

**问题：为什么不用红黑树来实现索引，而是用B/B+Tree？**

答：红黑树实现索引，在内存中是可以接受的，但是对于MySQL磁盘IO，红黑树深度太深了，会导致磁盘IO次数过多。而B+Tree一般也就三四层。而且B+Tree的组织结构符合磁盘一次读取一个盘块的方式。

用B-Tree存在同样的问题，B-Tree无论是叶子节点还是非叶子节点都存储数据，这就会导致一页中能存放的键值减少，导致整棵树的高度增加。

![](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/B_plus_tree.png)

next-key lock为什么是左开右闭？猜测是和B+树索引结构有关系。

#### next-key lock

[link](https://www.cnblogs.com/zhoujinyi/p/3435982.html)

#### 自适应hash索引

#### 索引分类

![image-20220319091518965](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/index_class.png)



![image-20220319091633094](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_index_class.png)

![image-20220319091710492](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_index_choice.png)



**InnoDB: 聚集索引和非聚集索引**：

![image-20220319141508197](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_ornot_clustered_index.png)



#### 索引操作语法

**创建索引：**

```sql
CREATE [UNIQUE][FULLTEXT] INDEX index_name ON table_name (index_col_name, ...);  -- ... 表示一个索引可以关联多个字段
```

一个索引如果只关联了一个列，那么叫做单列索引；如果关联了多个字段，那么这个索引叫联合索引或者组合索引。**联合索引字段是有顺序关系的。**

**查看索引：**

```sql
SHOW INDEX FROM table_name\G;
```

\G：更改打印方式。

查看一个表中的所有索引。



**删除索引：**

```sql
DROP INDEX index_name ON table_name;
```

#### SQL性能分析

先查看具体SQL的执行频率，我们优化的目标主要是SELECT执行很多的数据库，而不是执行增删改很多的数据库。

##### SQL执行频率

``` sql
SHOW [SESSION|GLOBAL] STATUS LIKE 'Com_______'; --- 后面7个_
```

![image-20220319145057547](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/show_global_status.png)

##### 慢日志查询

默认不开启慢日志，需要在```/etc/my.cnf```中开启

![image-20220319145733378](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/slow_query_log.png)



##### profile详情

```sql
SHOW PROFILES；
```

查看所有PROFILES。

查看数据库是否支持profiles：

```sql
SELECT @@have_profiling;
```

打开：

```sql
SET profiling = 1;
```

![image-20220319150718963](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/show_profiles.png)

**查看指定query的profile：**

```sql
SHOW PROFILE FOR QUERY query_id；
```

![image-20220319150953565](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/show_profile_for_query.png)



**查看指定query的cpu使用情况：**

```sql
SHOW PROFILE CPU FOR QUERY query_id;
```



##### EXPLAIN执行计划

**使用方法：**直接在SELECT语句前加上EXPLAIN/DESC

![image-20220319153050364](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/explain_select.png)

**各字段的含义：**

![image-20220319185110900](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/explain_col_expression.png)

![image-20220319185317759](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/explain_col_expression2.png)



**需要重点关注的字段：**

- type：根据type可以大致知道一条SQL的性能，按性能从高到低排序如下：
  - NULL：
  - system
  - const
  - eq_ref： 与ref类似，区别是使用唯一索引，即索引列的值唯一。
  - ref      ： 该索引列的值不唯一
  - range ： 有范围的索引扫描
  - index ： 全索引扫描
  - all       ： 全表扫描

- possible_key：可能应用的索引
- key：实际应用的索引
- key_len：表示索引使用的字节数，根据这个值可以判断索引的使用情况,特别是在组合索引的时候,判断该索引有多少部分被使用到非常重要。
- extra：

#### 索引使用

**最左前缀法则：**

[最长前缀法则](https://www.cnblogs.com/ljl150/p/12934071.html)

对于联合索引，要遵守最左前缀法则。最左前缀法则值得是查询从索引的最左列开始，并且不跳过索引中的列。<u>如果跳过了某一列，索引将部分失效。</u>

为什么会失效？ 

因为InnoDB索引的组织方式是B+树，且B+树键值顺序与定义索引时的顺序是一致的，查找索引的过程就是比较的过程，如果某个列跳过了，那么是没法进行比较的。比如建立时使用的a、b、c三个列，查找索引的过程会先根据a比较，如果a相同再比较b，以此类推。如果查询的时候没有使用a，显然是无法使用索引的。而如果使用了ac没有使用b，则会导致索引部分失效，只会使用a的索引。



**范围查询：**

 联合索引中，出现范围查询（>,<），范围查询右侧的列索引将失效。在业务允许的情况下，尽量使用`>=`或者`<=`。

|colA|colB|colC|，ABC列上有联合索引，假设使用`SELECT * FROM tb_name WHERE colA = "xxx" and colB > 10 and colC = "xxxx";`来查询，会导致colC上的索引失效。



**索引列运算：**

在索引列上进行运算操作，索引将失效。

```sql
SELECT * FROM tb_name WHERE SUBSTRING(col, 1, 2) = "a";
```

会导致col上的索引失效。



**字符串不加引号：**

对应的索引会失效。



**模糊查询：**

```sql
SELECT ... FROM table_name WHERE col LIKE "...%";
```

如果是尾部匹配，索引不会失效，如果是头部匹配，索引会失效。



**or连接的条件：**

用or分割开的条件，如果or前的条件中的列有索引，而后面的列中又没有索引，那么涉及到的索引都不会用到。也就是说or连接的所有列都得有索引。



**数据分布影响：**

如果MySQL评估使用索引比全表扫描性能还差，那么就不会使用索引。



**SQL提示：**

指定使用哪个索引。

USE INDEX:建议使用索引。

```sql
SELECT col FROM table_name USE INDEX (idx_name) WHERE ...;
```



IGNORE INDEX:忽略哪个索引。

```sql
SELECT col FROM table_name IGNORE INDEX (idx_name) WHERE ...;
```



FORCE INDEX:强制使用哪个索引。

```SQL
SELECT col FROM table_name FORCE INDEX (idx_name) WHERE ...;
```



**覆盖索引：**

SELECT中尽量使用覆盖索引，避免使用SELECT *。  回表查询：现在二级索引找键值，找到后再到聚集索引查找。

因为如果SELECT cols中cols覆盖了索引的话，直接就能在二级索引（非聚集索引）中查找到所有的值，就不需要回表查询了；相反的，如果包含了非索引列的话，就需要回表查询数据了。

Q：

一张表有四个字段(id, username, password, status)，由于数据量大，需要对以下SQL进行优化，如何优化？id是主键。

```sql
SELECT id, username, password FROM tb_user WHERE username = 'xxx';
```

回答：在username和password上建立联合索引。该索引为二级索引，二级索引叶子节点上就有要的id，因此不需要再在id上建联合索引。



**前缀索引：**

当字段类型为字符串（varchar、text等）时，有时候需要索引很长的字符串，这会让索引变得很大，查询时，浪费大量的磁盘IO，影响查询效率。此时可以只将字符串的一部分前缀建立索引，这样可以大大节约索引空间，从而提高索引效率。

语法：

```sql
CREATE INDEX idx_name ON table_name (column(n));
```

前缀长度：

可以根据索引的选择性来决定，而选择性是指不重复的索引值（基数）和数据表的记录总数的比值，索引选择性越高查询效率越高，唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。



**单列索引与联合索引选择：**

单列索引：一个索引只包含了单个列。

联合索引：一个索引包含了多个列。

如果存在多个查询条件，考虑针对于查询字段建立索引时，建议建立联合索引而非单列索引。

#### 建立索引原则

![image-20220319220625153](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/create_index_role.png)

### 锁

#### 全局锁

对整个数据库实例加锁，加锁后整个实例就处于只读状态，后续的DML的写语句，DDL语句，已经更新操作的事务提交语句都将被阻塞。典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性视图，保证数据的完整性。

#### 表级锁

每次操作锁住整张表。粒度大，发生锁冲突的概率高，并发度低。

分为三类：

- 表锁

  - 表共享读锁

  - 表独占写锁

    ```sql
    --- 获取锁
    LOCK TABLES table_name... READ/WRITE
    --- 释放锁
    UNLOCK TABLES
    ```

- 元数据锁（meta data lock，MDL）：系统自动控制，无需显示使用。维护表数据的数据一致性，在表上有活动事务时，不允许对元数据进行写入操作。

- 意向锁：避免执行DML时，加的行锁与表锁冲突，引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。

  - 意向共享锁（IS）：由语句```SELECT ... LOCK IN SHARE MODE```向表上添加IS锁。与表锁共享锁（read）兼容，与表锁排它锁（write）互斥。
  - 意向排他锁（IX）：由```INSERT、UPDATE、DELETE、SELECT ... FRO UPDATE```来添加。与表锁共享锁（read）及排它锁（write）都互斥。意向锁之间不会互斥。

![image-20220319222324636](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/yixiang_lock.png)

#### 行级锁

每次操作锁住对应的数据行。锁的粒度最小，发生冲突的概率最低，并发度最高。

InnoDB的数据是基于索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的锁。

行级锁主要分以下三类：

- 行锁（Record Lock）：锁定单个记录的锁，防止其他事务UPDATE和DELETE。在RC（Read Commit）、RR（Read Repeatable）隔离级别下都支持。

  - 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁。

  - 排它锁（X）：允许获取排他锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排它锁。
    ![image-20220319224436540](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/S_X_lock_compatiable.png)

    ![image-20220319224637859](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/sql_dml_lock_type.png)

- 间隙锁（Gap Lock）：锁定索引记录**间隙**（不含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读。在RR隔离级别下都支持。间隙锁是可以共存的。

  ![image-20220319224041757](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/gap_lock.png)

- 邻键锁（Next-Key Lock）：行锁和间隙锁的组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。

  ![image-20220319224233550](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/next_key_lock.png)

 

默认情况下，InnoDB在RR事务隔离级别允许，使用next-key lock进行搜索和索引扫描，以防止幻读。

**以下是需要注意的几点：**

- 针对**唯一索引**进行检索时，对<u>已存在的记录</u>进行等值匹配时，将自动优化为行锁。

- InnoDB的行锁是针对于索引加的锁，**不通过索引条件检索数据**，那么InnoDB将对表中的所有记录加锁，此时就会**升级为表锁。**

- 针对**唯一索引**进行检索时，给<u>不存在的记录</u>加锁时，优化为间隙锁。

- 普通索引上的等值查询，向右遍历时最后一个值不满足查询需求时，next-key lock退化为间隙锁。因为是非唯一的，所以需要向后扫描，加Gap锁，防 止出现幻读。

  ![image-20220319232314635](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/next_key_lock2_gap_lock.png)

- **唯一索引**上的范围查询，会访问到不满足条件的第一个值为止。



**以上所有情况，都是为了保证事务的ACID特性，最主要的是为了防止幻读！！！**

### InnoDB引擎

#### 逻辑存储结构

![image-20220319233554421](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/innodb_logical_storage.png)

#### 架构

[InnoDB内存](https://www.bilibili.com/video/BV1Kr4y1i7ru?p=134)

#### 事务原理

![image-20220319234252409](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/transcation_impl.png)

- redo log：重做日志，记录的是事务提交时数据页的物理修改，用来实现事务的持久性。由两部分组成

  - 重做日志缓冲（redo log buffer）以及重做日志文件（redo log file），前者在内存中，后者在磁盘中。当事务提交之后会把所有的修改信息都存在该日志文件中，用于在刷新脏页到磁盘发生错误时，进行数据恢复使用。

    ![image-20220319235405749](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/redo_log.png)

- undo log：用于记录数据被修改前的信息，作用包含两个：提供回滚和MVCC（多版本并发控制）。undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log就会记录一条相应的insert记录。反之亦然，当update一条记录时，会有一条相反的update记录写入undo log。当执行rollback时，就可以从rollback中的逻辑记录读取到的相应的内容进行回滚。

  - undo log销毁：undo log在事务执行时产生，事务提交时，并不会立即删除undo log，因为这些日志可能还用于MVCC。```INSERT```产生的undo log只有在回滚时需要，事务提交后可以被立即删除。```UPDATE```，```DELETE```产生的undo log，不仅回滚时需要，而且快照读时也需要，因此不能立即删除。
  - undo log存储：采用段的方式进行管理和记录，存放在undo segment中。

#### MVCC

[MVCC](https://blog.csdn.net/DILIGENT203/article/details/100751755?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&tdsourcetag=s_pctim_aiomsg)

依靠隐藏字段+undo log版本链+ReadView实现。

![image-20220320141333054](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/acid.png)

**相关概念：**

- 当前读：读取到的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。对于```SELECT ... LOCK IN SHARE MODE;```  ```SELECT ... FOR UPDATE;``` ```UPDATE```;```INSERT```;```DELETE```都是当前读。
- 快照读：简单的```SELECT```（不加锁）就是快照读，快照读读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。
  - Read Committed：每次```SELECT```都会产生一个快照读。
  - Repeatable Read：开启事务后第一个```SELECT```语句才是快照读的地方。
  - Serializable：快照读会退化为当前读。

**记录当中的隐藏字段：**

- DB_TRX_ID		：最近修改事务ID，记录插入这条记录或最后一次修改该记录的事务ID。
- DB_ROLL_PTR  ：回滚指针，指向这条记录的上一个版本，用于配合undo log，指向上一个版本。
- DB_ROW_ID     ：隐藏主键，如果表结构没有主键，将会生成该隐藏字段作为主键。



**undo log版本链：**

![image-20220320135008622](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/undo_log_version_chain.png)



**ReadView：**

ReadView（读视图）是快照读SQL执行时MVCC提取数据的依据，记录并维护当前活跃事务（未提交事务）的ID。

ReadView包括四个核心字段：

- m_ids：创建时活跃事务id集合
- min_trx_id：创建时最小活跃事务id
- max_trx_id：创建时预分配事务id，当前最大事务id+1
- creator_trx_id：ReadView创建者的事务id。



**ReadView的访问规则：**

- 如果记录上的事务id等于ReadView记录的创建者事务id，可以访问，说明这条记录是当前事务修改的

- 如果记录上的事务id小于ReadView记录的最小活跃事务id，可以访问，说明这个事务早已经提交了
- 如果记录上的事务id大于ReadView记录的最大活跃事务id，不可以访问，说明这个事务是在“未来”开启的，无论如何当前事务不能访问这个版本。
- 如果记录上的事务id处于ReadView记录中的最小活跃事务id和最大事务id的话，需要判断记录上的事务id是否是和自己并行的，即看一下它是否在ReadView中活跃事务id集合里，如果在，那么不能访问这条数据，如果不在，说明它已经提交了，因此是可以访问这条记录的。

[MySQL：基于undo log多版链条实现的ReadView机制，到底是什么](https://blog.csdn.net/zhizhengguan/article/details/122103389?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.pc_relevant_paycolumn_v3&spm=1001.2101.3001.4242.2&utm_relevant_index=3)

https://www.cnblogs.com/jmliao/p/13204946.html

trx_id：代表的是记录上的trx_id

![image-20220320135753243](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/readview_vist_role.png)

不同隔离级别，生成ReadView时机不同：

- Read Committed：在事务每一次执行快照读时生成ReadView
- Read Repeatable：仅在事务第一次执行快照读时生成ReadView，后续复用该ReadView。
- 当两次快照读之间存在当前读时，ReadView会重新生成（有可能产生幻读）。

![image-20220320140943482](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/RC_readview_vist.png)



#### redo log、undo log、binlog

[MySQL中的 redo 日志文件](https://www.cnblogs.com/hapjin/archive/2019/09/28/11521506.html)

- redo log、binlog提交过程
  - 新的数据更新到内存写入到redo log中，处于prepare阶段
  - 写入binlog
  - 提交事务，commit写入redo log
  - 两阶段提交是为了保证redo log与binlog的一致性。假设先写redo log，如果还没有写binlog系统就发生了异常，那么binlog里的记录可能是缺失的，当数据库恢复时数据库中是有对应的数据或者更新操作的，而binlog里没有，这个时候使用binlog同步从库或者恢复临时库的话，就会出现不一致状态。先写binlog是一样的。

- redo log和undo log是InnoDB引擎特有的，而binlog是MySQL级别的日志，与引擎无关。
- redo log是对执行结果的保存，好处是恢复时比较快，避免了重复计算。

- redo log - 保证了事务的持久性（Redo log怎么保证事务持久性的？WAL，先写日志）

  - **为什么要先写redo log再写磁盘WAL(write-ahead logging)？**

    因为先写redo log可以防止数据丢失。redo log可以设置成每次事务提交时就同步到磁盘，这样就可以防止数据丢失了。此外redo log是有buffer的，在buffer里进行操作要比直接提交事务时将修改同步到磁盘IO开销要低很多。

  - **每次修改数据时直接同步到磁盘不也可以防止数据丢失吗？**

    一个事务可能涉及到磁盘上许多个页，而这些页又很有可能是在物理外存储上是不连续的，使用随机IO读写性能开销是很难接受的，而redolog会现在内存缓冲区写入，后面刷入到硬盘，而且redolog的组织格式应该是物理连续（我猜的），而我们知道不管是机械硬盘还是固态硬盘，顺序读写速度是要比随机读写快很多的。这是一点。同时也是因为这个原因，导致直接将事务的修改同步到磁盘上这个操作开销是比较大的，也就是过程会比较长，如果在这个过程中出现了问题比如说断电了，那数据库就处于不一致状态了，数据也丢失了，所以说直接同步到硬盘是不怎么能防止数据丢失的。而用redo log的话就可以避免这个问题。

  - **每次commit后就执行fsync同步日志缓冲到磁盘这个过程也是开销比较大的**

    - 插入10W条数据，每次提交刷到磁盘耗时15s左右，每秒刷到磁盘耗时3s左右，每秒刷到os buffer和磁盘耗时2s左右。
    - 但是后面两种方式宕机时可能丢失大量数据。

- 有了redo log为什么还需要binlog？

  redo log大小是固定的，只能保存一段时间内的数据库信息变化，而binlog可以保存很长时间里的数据库DML、DDL执行语句。

  二是因为redo log是InnoDB独有的，其它引擎是没有的。

- binlog

  - 记录对数据库所有修改的操作，主要用于主从复制，也可以临时从binlog恢复数据库。binlog是MySQL级别的。

- undo log

  - 提供回滚和多版本控制（MVCC）
  - undo log和redo log记录物理日志不一样，它也是逻辑日志。**可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。**
  - 

#### MySQL是先写redo log还是先写binlog，哪个又先commit？

- 会话发起commit操作
- 存储引擎层开启[Prepare]状态：在对应的Redo日志记录上打上Prepare标记
- 写入binlog并执行fsync(刷盘)
- 在redo日志记录上打上COMMIT标记表示记录提交完成

binlog只有事务提交前会

#### 为什么先写redo log



## 问题

### MySQL事务处理 - 银行转账问题

美团一面问的，没咋听明白问的意思。先问用MySQL实现一个银行转账系统，隔离级别选哪个，答RR级别。然后又继续问就没咋听明白想问啥了。

事务的作用主要是在服务器发生错误或者崩溃的情况下确保数据的一致性。事务是一个或者一系列的查询，这些查询要么全部执行要么执行要么全部不执行。

### 为什么不建议使用外键约束

外键的优点

- 保证数据的完整性和一致性
- 级联操作方便
- 将数据完整性判断托付给了数据库完成，减少了程序的代码量

然而，鱼和熊掌不可兼得。外键是能够保证数据的完整性，但是会给系统带来很多缺陷。

- 性能问题：假设一张表名为user_tb。那么这张表里有两个外键字段，指向两张表。那么，每次往user_tb表里插入数据，就必须往两个外键对应的表里查询是否有对应数据。如果交由程序控制，这种查询过程就可以控制在我们手里，可以省略一些不必要的查询过程。但是如果由数据库控制，则是必须要去这两张表里判断。
- 并发问题：在使用外键的情况下，每次修改数据都需要去另外一个表检查数据,需要获取额外的锁。若是在高并发大流量事务场景，使用外键更容易造成死锁。
- 扩展性问题：
  - 做平台迁移方便，比如你从`Mysql`迁移到`Oracle`，像触发器、外键这种东西，都可以利用框架本身的特性来实现，而不用依赖于数据库本身的特性，做迁移更加方便。
  - 分库分表方便，在水平拆分和分库的情况下，外键是无法生效的。将数据间关系的维护，放入应用程序中，为将来的分库分表省去很多的麻烦。
- 技术问题：使用外键，其实将应用程序应该执行的判断逻辑转移到了数据库上。那么这意味着一点，数据库的性能开销变大了，那么这就对DBA的要求就更高了。很多中小型公司由于资金问题，并没有聘用专业的DBA，因此他们会选择不用外键，降低数据库的消耗。相反的，如果该约束逻辑在应用程序中，发现应用服务器性能不够，可以加机器，做水平扩展。如果是在数据库服务器上，数据库服务器会成为性能瓶颈，做水平扩展比较困难。

# Spring相关

## IOC

### Spring IOC的实现原理

Java原生对象创建过程：

- 检查类是否加载，如果未加载，先加载类对象
- 为对象分配内存
- 初始化零值
- 设置对象头
- 执行\<init>方法，这一步开始对象才真正的在Java层面创建。

Spring Bean创建过程：

- 

### Bean的作用域

- singleton：单例
- prototype：原型
- request：
- session：
- global-session：

#### 单例模式是否线程安全

不一定，和单例类的写法有关。如果这个单例的对象是无状态的那么就线程安全，否则线程不安全。

### Bean的生命周期

在spring框架中，所有的bean对象都有生命周期，就是指bean的创建、初始化、服务、销毁的一个过程。



### @Autowaired和@Resource的区别

### Spring循环依赖

利用二级缓存和三级缓存。

- 一级缓存：存放完全初始化的Bean。
- 二级缓存：存放原始的Bean对象，尚未进行属性填充。考虑的是性能。
- 三级缓存：存放Bean工厂对象。考虑的是代理。



**单例**bean的依赖注入分为构造器注入和setter方法注入。

- Spring只会解决setter方法注入的循环依赖，构造器注入的循环依赖会抛BeanCurrentlyInCreationException异常。
- Spring不会解决prototype作用域的bean，因为Spring容器不进行缓存"prototype"作用域的bean，因此无法提前暴露一个创建中的bean。如果有循环依赖会抛BeanCurrentlyInCreationException异常。



## AOP

### JDK实现动态代理和CG LIb的区别

1）静态代理：实现简单，但是只能为一个类服务，如果需要代理大量的类，就需要写很多的代理类。通过编码实现。

2）动态代理：需要实现接口，通过反射实现。

JDK动态代理使用过程：

- 类实现接口
- 在实现InvocationHandler接口的类中对包装类进行代理
- 调用Proxy.newProxyInstance()得到被代理的类实例

3）CG Lib：CG Lib不需要类实现接口



## 事务

事务的传播性一般用在事务嵌套的场景，比如一个事务方法里面调用了另外一个事务方法，那么两个方法是各自作为独立的方法提交还是内层的事务合并到外层的事务一起提交，这就是需要事务传播机制的配置来确定怎么样执行。
常用的事务传播机制如下：

- PROPAGATION_REQUIRED：Spring默认的传播机制，能满足绝大部分业务需求，如果外层有事务，则当前事务加入到外层事务，一块提交，一块回滚。如果外层没有事务，新建一个事务执行。
- PROPAGATION_REQUES_NEW：该事务传播机制是每次都会新开启一个事务，同时把外层事务挂起，当当前事务执行完毕，恢复上层事务的执行。如果外层没有事务，执行当前新开启的事务即可
- PROPAGATION_SUPPORT：如果外层有事务，则加入外层事务，如果外层没有事务，则直接使用非事务方式执行。完全依赖外层的事务。
- PROPAGATION_NOT_SUPPORT：该传播机制不支持事务，如果外层存在事务则挂起，执行完当前代码，则恢复外层事务，无论是否异常都不会回滚当前的代码
- PROPAGATION_NEVER：该传播机制不支持外层事务，即如果外层有事务就抛出异常
- PROPAGATION_MANDATORY：与NEVER相反，如果外层没有事务，则抛出异常
- PROPAGATION_NESTED：该传播机制的特点是可以保存状态保存点，当前事务回滚到某一个点，从而避免所有的嵌套事务都回滚，即各自回滚各自的，如果子事务没有把异常吃掉，基本还是会引起全部回滚的。



Spring事务实现原理：



# Java

## Java基础

### Java内部类

内部类分为：

- 静态内部类

因为是static，所以不依赖于外围类对象实例而独立存在，**静态内部类的可以访问外围类中的所有静态成员，包括`private`的静态成员**。同时静态内部类可以说是所有内部类中独立性最高的内部类，其创建对象、继承（实现接口）、扩展子类等使用方式与外围类并没有多大的区别。

- 成员内部类

定义在类的内部，而且与成员方法、成员变量同级，即也是外围类的成员之一，因此 成员内部类 与 外围类 是紧密关联的。

- 局部内部类
- 匿名内部类

[内部类](https://mp.weixin.qq.com/s?__biz=MzI2OTQ4OTQ1NQ==&mid=2247484075&idx=1&sn=e0fd37cc5c1eb5fb359ed3dc9c15af66&scene=19#wechat_redirect)

### Java引用类型

- 强引用：不会被GC，用户代码指定的引用
- 软引用：在发生OOM之前会被GC
- 弱引用：会被GC
- 虚引用：不用管，接触不到

### 泛型

泛型有三种使用方法：泛型类、泛型接口、泛型方法。

泛型方法需要将泛型修饰符放在函数修饰符之后。

```java
public static <T> T fun(T typeInstance) {};
```

下面的三个通配符，只能用在具体的类引用上（可以做形参）：

<?>通配符，用在具体的泛型类上，```ArrayList<?> list```这类的泛型不能接收实参，且如果泛型作为返回值，那么返回值不强制转型的情况下只能赋给Object。

____

[[Java中和的理解]](https://www.cnblogs.com/chenxibobo/p/9655236.html)

<? extends T>适用于GetFirst，上界`<? extends T>`不能往里存，只能往外取。不能往里存代表匹配形参时，只能匹配null，不能匹配任何其他类型的实参；只能往外取表示匹配返回值时，会向上转型为T。

**为什么呢**？

因为比如List<? extends T>表示“任何从T继承的类型的列表”，编译器是无法根据这些信息确定List所具体持有的类型，因此往里存是一件不安全的事。比如说我本来有一个List\<Interger\>，现在我将它赋值给List<? extends Number>，如果我这个时候往里存Long的值，那么不就出错了吗。尽管Long也是Number的子类。而作为返回值就没这一顾虑了。说到底泛型在Java里很大程度上还是为了集合服务的，而JDK1.5之前又没有泛型，这应该是Java为了兼容以前的代码又想要泛型带来的优点而不得不做出的牺牲。



<? super T>适用于PutFirst，做形参时可以传入T和T的子类实参，返回时类型丢失。

super只能添加T和T的子类，不能添加T的父类,返回值向上转型为Object。

**为什么呢？**

List<? super T>表示这个list中存放的是T或者是T的某个超类，那么显然我往里面存T是可以的，存T的子类也同意的可以的，都是兼容的。但是返回时就有问题了，只要是T的超类都可以往这个List里存，那么显然Object类对象也是可以往里存的，那么我往外取的时候，显然是不能把Object强制转型成别的类型的，最终的结果就是<? super T>做返回值匹配时，只能匹配为Object。



最后看一下什么是PECS（Producer Extends Consumer Super）原则，已经很好理解了：

- 频繁往外读取内容的，适合用上界Extends。
- 经常往里插入的，适合用下界Super。

__________

①extends后面跟的类型如<任意字符 extends 类/接口>表示泛型的上限

<? extends T>表示泛型的通配符必须是T的某个子类，最高上限是T。

在类上做修饰时，不能使用super来限制泛型下限（没有意义，一个能存Object，但是不能存Integer的东西？）。

```java
import java.util.*;
class Demo<T extends AbstractList>{}
public class Test{
    public static void main(String[] args) {
	Demo<ArrayList> p = null; // 编译正确
//这里因为ArrayList是AbstractList的子类所以通过
//如果改为Demo<AbstractCollection> p = null;就会报错这样就限制了上限
    }
}
```

②同样的super表示泛型的下限

<? super T>表示泛型的通配符必须是T的某个超类，最低下限是T。

在类上做修饰时，不能使用super来限制泛型下限（没有意义，一个能存Object，但是不能存Integer的东西？）。

```java
class Demo<T> {
    Comparator<? super T> comparator;

    public Demo(Comparator<? super T> comparator) {
        this.comparator = comparator;
    }
}

public class Test {
    public static void main(String[] args) {
      	// 这两条编译不报错
        Demo<Integer> demo = new Demo<>(new MyNumberComparator());
        demo = new Demo<>(new MyIntegerComparator());

        // 下面两条编译报错
        demo = new Demo<>(new MyStringComparator());
        demo = new Demo<>(new MyLongComparator());
    }
}

class MyNumberComparator implements Comparator<Number> {
    
    @Override
    public int compare(Number o1, Number o2) {
        return 0;
    }
}

class MyStringComparator implements Comparator<String> {

    @Override
    public int compare(String o1, String o2) {
        return 0;
    }
}

class MyIntegerComparator implements Comparator<Integer> {

    @Override
    public int compare(Integer o1, Integer o2) {
        return 0;
    }
}

class MyLongComparator implements Comparator<Long> {

    @Override
    public int compare(Long o1, Long o2) {
        return 0;
    }
}
```





### String去除空白字符

```java
str.trim(); 													// 去除首尾空白字符 
str.replace(old, new);								// 将old替换为new， old和new既可以是char，也可以是String
str.replaceAll(regex, replacement); 	// 将匹配regex的串替换为replacement
```

#### 使用正则表达式替换 //TO-DO:

### 数组转List //TO-DO:

#### 基本数据类型数组

对于int、long、double

```	java
int[] arr = {1, 2, 3, 4, 5};
Interger arrWarped = Arrays.stream(arr).boxed().toArray(Integer[]::new);
```

其他基本类型：

循环。

#### 对象数组



### 注解

[深入理解Java注解类型(@Annotation)](https://blog.csdn.net/javazejian/article/details/71860633)

### 双亲委派

#### 双亲委派机制有什么作用



#### 为什么要打破双亲委派机制

#### 怎么打破双亲委派机制

## IO 

总结BIO和NIO的区别：

NIO就是延迟IO，只有当真正的IO事件发生时，才去读写，其余时间有selector来轮询。

而BIO就是无延迟IO，不管是否有IO事件发生了，都需要线程阻塞在调用的地方。

### BIO

- 同步阻塞IO。一个连接一个线程，当没有数据可读时，需要阻塞等待。
- BIO面向字节流和字符流的。

- accept() 阻塞的，只有来了连接才会返回
- read()是阻塞的，只有有数据可读才返回
- write()是阻塞的，只有对方接收了数据才返回。

BIO缺点：

- 请求很多时，需要很多的线程来处理，因为每个连接都可能导致某个线程阻塞。

BIO里的流：

- InputStream：输入字节流。抽象类，基于字节的输入操作，是所有输入流的父类。定义了所有输入流都具有的共同特征。
- OutputStream：输出字节流。抽象类。基于字节的输出操作。是所有输出流的父类。定义了所有输出流都具有的共同特征。
- Reader：字节输入流。抽象类，基于字符的输入操作。
- Writer：字节输出流。抽象类，基于字符的输出操作。

- File：文件类。用于文件或者目录的描述信息，例如生成新目录，修改文件名，删除文件，判断文件所在路径等。
- RandomAccessFile：随机存储文件类型。直接继承至Object.它的功能丰富，**可以从文件的任意位置进行存取（输入输出）操作**。

### NIO

- IO调用不会阻塞，基于Reactor。

- 同步非阻塞IO。一个线程处理多个连接，有一个线程负责管理连接的建立，其他线程去

-  NIO面向块。面向Channel和Buffer。数据总是从缓冲区写入到通道，或者从通道写入缓冲区。
- I/O多路复用，I/O就是指的我们网络I/O,多路指多个TCP连接(或多个Channel)，复用指复用一个或少量线程。串起来理解就是很多个网络I/O复用一个或少量的线程来处理这些连接。

NIO三大核心：Channel、Buffer、Selector

____

**Buffer缓冲区**

 缓冲区本质上是一块可以写入数据，然后可以从中读取 数据的内存。这块内存被包装成NIO Buffer对象，并提供 了一组方法，用来方便的访问该块内存。相比较直接对 数组的操作，Buffer API更加容易操作和管理。

- ByteBuffer
- CharBuffer
- DoubleBuffer
- FloatBuffer
- IntBuffer
- LongBuffer
- ShortBuffer
- MappedByteBuffer

Buffer操作：

- 将数据写入buffer
- 调用flip()反转读写模式
- 从缓冲区读取数据
- buffer.clear()，buffer.compact()

**Buffer原理：**

三个关键属性：

- capacity：buffer的大小
- position：写模式-当前写入位置，
- limit：

API：

- flip()实际上就是修改这几个值的位置

- rewind() 将position设为0，limit不变
- clear() 清空buffer
- compact() 只清空读过的数据。

- mark() reset() 标记position，reset返回

**缓冲区操作：**

1. 缓冲区切片 slice()从原来的缓冲区中切分出子缓冲区，分片前需要先设置position、limit的值。 
2. 只读缓冲区 asReadOnlyBuffer()
3. 直接缓冲区, allocateDirect()，跳过虚拟机，直接在物理主机上分配一块内存。 
   1. 直接缓冲区：直接在物理内存分配，不需要从内核空间复制到JVM内存。
   2. 非直接缓冲区：在JVM中分配内存，每次操作需要在内核空间和JVM内存里复制数据。
4. 内存映射文件IO：channel.map(mode, start, size)，返回MappedByteBuffer



____

**Channel（通道）**

 Java NIO的通道类似流，但又有些不同：既可以从通道 中读取数据，又可以写数据到通道。但流的（input或output)读写 通常是单向的。 通道可以非阻塞读取和写入通道，通道可 以支持读取或写入缓冲区，也支持异步地读写。

- FileChannel，从文件中读写数据。
- DatagramChannel，能通过UDP读写网络中的数据。
- SocketChannel，能通过TCP读写网络中的数据。
- ServerSocketChannel，可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。



**FileChannel:**

打开FileChannel：

- 需要通过InputStream、OutputStream、RandomAccessFile来打开FileChannel。

FileChannel读写数据：

- 调用read(buffer)方法将数据读取到缓冲区，返回值为读取了多少数据，如果返回-1表示无数据返回
- 调用write(buffer)将数据写到缓冲区

API：

- position() 返回当前读写位置
- position(int loc) 设置当前读写位置
- size() 关联的文件的大小
- truncate(int size)  截取文件前size字节大小的部分

- force() 强制写磁盘
- transferTo(Channel c， int start, int end)  将数据传输到另一个通道
- transferFrom(Channel c, int start, int end) 从另一个通道复制数据



**Socket通道:**

- DatagramChannel和SocketChannel实现读写。
- ServerSocketChannel监听连接和创建新的SocketChannel对象，本身不传输数据。

Socket通道既可以运行在阻塞模式，又可以运行在非阻塞模式。



**ServerSocketChannel:**

- 是一个监听器，监听连接



**SocketChannel**

- 是Tcp连接的封装
- 实现了可选择通道，实现了多路复用。



API:

- open()
- isOpen()
- isConnected()
- isConnectionPending()
- finishConnect()
- configureBlocking(boolean)
- setOption() 配置tcp信息，比如keep_alive
- getOption



**DatagramChannel:**

- 面向UDP，无连接

____

**Scatter/Gather:**

- Scatter:从Channel读取数据到多个Buffer，

- Gather:将多个Buffer中的数据写入Channel
- read(ByteBuffer[] arr)  write(ByteBuffer[] arr)

____

**Selector选择器**

- Selector： epoll实例（数据结构）
- 注册register：epoll_ctl，添加socket到epoll数据结构中，需要绑定感兴趣的事件
- select ： epoll_wait，阻塞，直到发生感兴趣的事才返回。

 Selector是 一个Java NIO组件，能够**检查一个或多个 NIO 通道**，并确定哪些通道已经准备好进行读 取或写入。这样，**一个单独的线程可以管理多 个channel，从而管理多个网络连接**，提高效率。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便。

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/overview-selectors.png)

**SelectableChannel:**

- FileChannel不是
- ServerSocketChannel、SocketChannel都是



Channel需要注册到Selector才能被轮询。epoll_ctl()

- Selector查询的是某个事件的就绪状态，底层是通过epoll中注册的回调函数实现的，由操作系统来完成。比如网口来了一条TCP连接请求，OS kernel肯定是最先知道的，这个时候它再通过调用回调函数通知epoll数据结构，让他把红黑树上对应的节点移到就绪队列。
- 需要Channel必须处于非阻塞状态。



API:

- select() ：阻塞，直到注册的事件发生
- select(long timeout)：阻塞，直到注册的事情发生或者超时
- selectNow()：不阻塞，直接返回
- wakeup()：让处于select阻塞的线程唤醒
- close()：唤醒所有阻塞线程，注销所有Channel（不会关闭Channel），销毁所有SelectionKey

### Reactor

#### 单Reactor单线程

![图片](/Users/hq/Desktop/面试/reactor_sigle_thread.png)

- Reactor负责监听和分发事件
- Acceptor负责获取连接
- Handler负责业务处理

处理过程：

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

缺点是

- 无法有效利用cpu多线程能力。

- 如果业务处理时间很长，会导致响应很慢

#### 单Reactor多线程

![图片](/Users/hq//Documents/md_image/reactor_multi_thread.png)

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；

上面的三个步骤和单 Reactor 单线程方案是一样的，接下来的步骤就开始不一样了：

- Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
- 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。

「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**。

#### 多Reactor多线程

![图片](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/640-20220324100501060.png)

方案详细说明如下：

- 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept  获取连接，将新的连接分配给某个子线程；
- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。
- 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。

采用了「多 Reactor 多进程」方案的开源软件是 Nginx，不过方案与标准的多 Reactor 多进程有些差异。

具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接，通过锁来控制一次只有一个子进程进行 accept（防止出现惊群现象），子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程。



## Java容器

### 有哪些容器

collections和map两种基本的。

collections又包括list、set、queue。

list：ArrayList、LinkedList

set：TreeSet、HashSet、LinkedHashSet

queue：LinkedList（Dqueue）、PriorityQueue（二叉堆）

Map：HashMap、TreeMap、LinkedHashMap

### 快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？

**快速失败：**

在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。

**原理：**迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变 modCount 的值。每当迭代器使用 hashNext()/next() 遍历下一个元素之前，都会检测 modCount 变量是否为 expectedmodCount 值，是的话就返回遍历；否则抛出异常，终止遍历。



**安全失败：**

采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。

**原理：**由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发 Concurrent Modification Exception。

在java.util.concurrent 并发包的集合，如 ConcurrentHashMap, CopyOnWriteArrayList等，默认为都是安全失败的。

#### Map K V问题

| 名称              | K是否可以为null | V是否可以为null |
| ----------------- | --------------- | --------------- |
| HashMap           | 是              | 是              |
| TreeMap           | 是              | 是              |
| ConcurrentHashMap | 否              | 否              |
| HashTable         | 否              | 否              |

为什么这样设计？

允许value为null的话，调用map.get(key)时，得到null会有二义性：

- 此key未映射过
- 此key的value被设置为了null

单线程环境下可以用map.containsKey(key)来避免二义性。

多线程环境下是不可行的，因为调用map.containsKey(key)后，可能有其它线程修改了这个key的映射关系，所以是不能够消除二义性的。

> 用反证法来推理，假设concurrentHashMap允许存放值为null的value。
>
> 这时有A、B两个线程。
>
> 线程A调用concurrentHashMap.get(key)方法,返回为null，我们还是不知道这个null是没有映射的null还是存的值就是null。
>
> 我们假设此时返回为null的真实情况就是因为这个key没有在map里面映射过。那么我们可以用concurrentHashMap.containsKey(key)来验证我们的假设是否成立，我们期望的结果是返回false。
>
> 但是在我们调用concurrentHashMap.get(key)方法之后，containsKey方法之前，有一个线程B执行了concurrentHashMap.put(key,null)的操作。那么我们调用containsKey方法返回的就是true了。这就与我们的假设的真实情况不符合了。也就是上面说的二义性。

### ArrayList相关

ArrayList底层是数组实现的，适合查询多，修改少的场景。

#### ArrayList扩容机制

每次add时检查，容量不足时，新容量为旧容量+旧容量*2

#### ArrayList循环时删除元素怎么删，fast-fail？

```java
list.add("a");
list.add("bb");
list.add("bb");
list.add("a");
list.add("a");

for (i = 0; i < list.size(); ++i) {.
    if (list.get(i) equals "bb") {
        list.remove(list.get(i));
    }
}
```

上面写法会导致有一个bb删不掉。

解决：

```java
for(int i=list.size()-1; i>-1; i--){
  if(list.get(i).equals("jay")){
    list.remove(list.get(i));
  }
}
OR
  
Iterator itr = list.iterator();
while(itr.hasNext()) {
      if(itr.next().equals("jay") {
        itr.remove();
      }
}
```

### LinkedList

LinkedList是用双向链表来实现的，适合修改多，查询少的场景。

常用API

实现队列

实现栈：

| 描述         | 方法                            |
| ------------ | ------------------------------- |
| 压栈         | push(e)     addFirst(e)         |
| 出栈并返回   | pop(e)                          |
| 查看栈顶元素 | getFirst()   element()   peek() |



### HashMap相关

#### HashMap实现原理

JDK1.7之前，HashMap是利用数组+链表实现的，基于头插法

JDK1.8之后，HashMap中的链表会在长度大于8且数组大小大于64时，转为红黑树，防止退化。

HashMap默认的初始化大小为 16。当HashMap中的元素个数之和大于负载因子*当前容量 的时候就要进行扩充，容量变为原来的 2 倍。(这里注意不是数组中的个数，而且数组中 和链/树中的所有元素个数之和!)

> 我们还可以在预知存储数据量的情况下，提前设置初始容量(初始容量 *=* 预知数据量 */* 加载因子)。这样做的好处是可以减少 *resize()* 操作，提高 *HashMap* 的 效率

#### HashMap为什么容量设计成2的n次方

计算Hash桶的方法是(n - 1)&hash，**<u>取余(%)操作中如果除数是2的幂次则等价 于与其除数减一的与(&)操作</u>**(也就是说 **hash%length==hash&(length-1)**的前提是 **length** 是**2**的 **n** 次方;)。**”** 并且 采用二进制位操作 **&**，相对于**%**能够提高运算效率， 这就解释了 **HashMap** 的长度为什么是**2**的幂次方。

#### HashMap.put(K, V)

1）根据key的hash值，通过(length - 1)&hash的来获取key应该存放到哪个hash桶里

2）如果这个桶里是空的，那么将KV打包成Entry放入即可。（1.8是Node）

3）如果发生了Hash冲突，那么解决冲突

- 如果是1.7，首先判断K是否已存在，存在的话就更新V，返回旧值；否则在put之前会先判断是不是需要扩容，如果不需要，使用头插法将封装好的KV Entry插入链表
- 如果是1.8，会先判断Hash桶上的节点类型，以便选择不同的算法来插入节点
  - 如果是红黑树的TreeNode，则添加到红黑树中，添加时判断是否K已存在，已存在就更新V
  - 如果是链表的Node，使用尾插法插入，同样需要遍历链表判断是否K已存在，插入后如果链表长度大于8，转为红黑树
  - 判断是否需要扩容

#### JDK1.7HashMap扩容死循环？为什么1.8里使用尾插法插入元素？

多线程环境下才会出现，不做深入了解。

#### 红黑树介绍

- 每个节点不是红色的就是黑色的
- 父子节点的颜色不能相同
- 所有的NIL叶子节点是黑色的
- 根节点是黑色的
- 从一个节点到NIL叶子节点包含相同多的黑色节点

### LinkedHashMap

- 继承自HashMap，内部维护双向链表
- Key排序，可以按插入顺序和访问顺序排序。如果是按访问顺序排序，那么get和put已经存在的数据时，会将数据移动到表尾。
- 

### TreeMap

使用红黑树实现二叉查找树来存储，因此Key是排好序的。也就是说，只有当我们需要将Key排序时，才考虑使用TreeMap。需要Key实现Comparable接口或者我们传入一个Comparator比较器。

| 描述                                       | 方法                       |
| ------------------------------------------ | -------------------------- |
| 获取第一个key                              | map.firstKey()             |
| 获取最后一个key                            | map.lastKey()              |
| 获取严格小于某个值的key                    | map.lowerKey(K key)        |
| 获取严格大于某个值的key                    | map.higherKey(K Key)       |
| 获取小于等于某个值的key                    | map.floorKey(K key)        |
| 获取大于等于某个key的key                   | map.ceilingKey(K key)      |
| 获取从fromKey到toKey的数据,左闭右开        | subMap(K fromKey, K toKey) |
| 获取从fromKey到结尾的所有数据，包含fromKey | tailMap(K fromKey)         |
| 获取从开头到toKey的所有数据，不包含toKey   | headMap(K toKey)           |

### 线程安全的集合类有哪些？

- Vector
- HashTable
- Stack

### TreeSet

使用TreeMap实现，value均指向同一个值。

### HashSet

使用HashMap实现，value指向同一个Object对象。

### Queue相关

**因为容量问题而导致操作失败后处理方式的不同** 可以分为两类方法: 一种在操作失败后会抛出异常，另一种则会返回特殊值。

| Queue方法 | 抛出异常  | 返回特殊值 |
| --------- | --------- | ---------- |
| 插入队尾  | add()     | offer      |
| 删除队首  | remove()  | poll()     |
| 查询队首  | element() | peek()     |

**add**    增加一个元索           如果队列已满，则抛出一个IIIegaISlabEepeplian异常
**remove**  移除并返回队列头部的元素  如果队列为空，则抛出一个NoSuchElementException异常
**element** 返回队列头部的元素       如果队列为空，则抛出一个NoSuchElementException异常
**offer**    添加一个元素并返回true    如果队列已满，则返回false
**poll**     移除并返问队列头部的元素  如果队列为空，则返回null
**peek**    返回队列头部的元素       如果队列为空，则返回null
**put**     添加一个元素           如果队列满，则阻塞
**take**    移除并返回队列头部的元素   如果队列为空，则阻塞

| Deque    | 抛出异常    | 返回特殊值 |
| -------- | ----------- | ---------- |
| 插入队首 | addFirst    | offerFirst |
| 插入队尾 | addLast     | offerLast  |
| 删除队首 | removeFirst | pollFirst  |
| 删除队尾 | removeLast  | pollLast   |
| 查询队首 | getFirst    | peekFirst  |
| 查询队尾 | getLast     | peekLast   |

Deque继承结构：

![image-20220308160617743](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/queue_hire.png)

#### PriorityQueue



## 多线程

### 多线程相关问题

#### Java线程状态

- NEW
- RUNNABLE
- BLOCKED
- WAITED
- TIME_WITED
- TERMINATED

#### ThreadLocal

ThreadLocal内存泄漏：Thread中有ThreadLocalMap的强引用，而ThreadLocalMap有对Entry数组的强引用，Entry数组有对Entry实例的强引用，而Entry类是继承WeakReference的，它是一个KV结构，其中K是ThreadLocal实例，而V是ThreadLocal中设置的值，Entry对ThreadLocal的引用是弱引用，因此当ThreadLocal被GC后，还存在一条强引用链指向value，而此时value已经不能被用户所访问了，此时发生了内存泄漏。

ThreadLocal本身也做了一些手段来避免内存泄漏。

sut的时候hash碰撞不会拉链法，而是往后找空，找空的过程中遇到key为空的就会清除过期的entry。



同一个ThreadLocal实例是可以被多个线程共享的，因为ThreadLocal本身是无状态的，ThreadLocalMap是每个线程独有的，Thread类中有ThreadLocalMap的引用，每个线程创建的都是独立的ThreadLocalMap实例。

```java
void createMap(Thread t, T firstValue) {
    t.threadLocals = new ThreadLocalMap(this, firstValue);
}
```

#### Java单例模式怎么写？

懒汉式：

```	java
public class SingletonInstance {
    private static volatile SingletonInstance instance;

    public static SingletonInstance getInstance() {
        if (instance == null) {
            synchronized (SingletonInstance.class) {
                if (instance == null) {
                    instance = new SingletonInstance();
                }
            }
        }
        return instance;
    }
}
```

为什么要双检锁？

假设第二次检测，那么就会出现这种情况：第一个线程已经获得监视器锁，但是还没来得及创建对象，那么这时候对象的引用仍然是null，此时又有个线程来请求单例对象，这时候它发现引用仍然为null，所以它也会去获得锁，这样就会有多个对象被创建出来，就不符合单例的情况了。

饥饿式：

public class SingletonInstance {
    private static volatile SingletonInstance instance;

```java
public class SingletonInstance {
    private static volatile SingletonInstance instance = new SingletonInstance();

    public static SingletonInstance getInstance() {
        return instance;
    }
}
```
}

#### AtomicInteger的ABA问题

加时间戳或者版本号，底层实现的，Java层面不需要关心。

### AQS相关

#### 什么是AQS

AQS是Java中AbstractQueuedSynchronizer的简称。AQS本质上是一个带有头节点的双向的CLH队列，CLH队列的意思就是等待节点会在前一个节点的属性上自旋等待锁。AQS中维护了一个volatile的变量state，在不同的类中，这个state的含义不同，比如在ReentrantLock中，state就表示当前是否有锁以及重入的次数，而在Semaphore中则代表许可的数量。AQS中通过大量的CAS手段来维护这个同步队列的一致性。

AQS中的每个节点有5种状态，后面的节点根据前面的节点的状态来决定自己是自旋的去获得锁还是阻塞。

此外，AQS中还有ConditionObject子类，通过这个子类AQS可以完成await、signal方法，并且此类只能在独占模式下使用，await/signal的语义与Object中的wait/notify是一致的，即执行wait/await要先获得锁，执行后线程释放锁然后阻塞，而执行signal/notify则是会唤醒阻塞的线程。ConditionObject自身维护了一个阻塞队列，wait/signal其实就是线程进入阻塞队列以及从阻塞队列中转移到同步队列中排队。

#### ReentrantLock

Sync有两种实现，一种是公平的，另一种是非公平的。

非公平的Sync实现中，线程执行tryAcquire时，如果此时state为0，那么线程会立即尝试CAS的把state修改成1，也就是去尝试获取锁。它不会管AQS的同步队列中是否还有线程在等待，这也是非公平的体现。

公平锁的实现中，执行tryAcquire时，如果state为0，会先判断同步队列中是否有线程在等待，如果有的话，自己也要乖乖的去排队，而如果没有的话，那么表示自己可以抢占锁，执行CAS尝试设置state。

默认实现是非公平的，因为公平模式下性能不如非公平好。

#### CountDownLatch

适用于一个线程要等待其它几个线程执行完才能继续运行情况，需要阻塞的线程执行await方法，其它线程执行countDown

基于AQS，构造时会把state变量设置成门票的数量，只有所有门票都用掉了，才能通过。每次执行countDown方法就会调用tryReleaseShared方法，会将state减少1，而await方法则是调用tryAcquireShared方法，这里只有当state为0时，才会返回1也就是不会去阻塞。

#### CyclicBarrier

适用于几个线程都达到一定的状态才能继续运行的情况（比如某些游戏加载机制）。

CyclicBarrier是用ReentrantLock加锁实现的，而不是直接使用AQS。

线程在等待时如果被中断了，那么栅栏也会被冲破。

#### Semaphore

信号量，是一种计数器，初始化时需要设置许可的数量。底层是借助AQS的共享模式实现的。Semaphore也有公平和非公平的实现，类似ReentrantLock

#### ReentrantReadWriteLock

借助AQS实现的，也实现了公平模式和非公平模式，可重入。和`ReentrantLock`不同的是`ReentrantReadWriteLock`分别提供了**写锁**和**读锁**，其中写锁是独占锁，而读锁则是共享锁。

其中锁的关系如下：

1. 如果已经有线程持有读锁，那么所有线程都不能获得写锁，但是可以继续获得读锁
2. 如果已经有线程持有写锁，那么其他线程都不能再次获得任何锁，只有持有写锁的线程可以获得读锁和写锁
3. 允许锁降级，即线程可以：获得写锁 -> 获得读锁 -> 释放写锁 -> 释放读锁
4. 不允许锁升级，即线程不可以：获得读锁 -> 获得写锁 -> 释放读锁

### 多线程容器相关

#### ConcurrentHashMap原理

jdk1.7中采用的是分段锁的设计，将Hash桶分成多个segment，每个segment有独立的锁，从而多线程访问不同的segment时不会出现锁争用的情形；另外jdk1.7中采用的存储方式也是hash桶+链表。

jdk1.8中取消了分段锁的设计，将锁的粒度更细化，使用了大量的synchronized和AQS操作来进行同步；jdk1.8中采用hash桶+链表/红黑树的结构，与HashMap一致。

[一文读懂Java ConcurrentHashMap原理与实现](https://zhuanlan.zhihu.com/p/104515829)

#### jdk1.8中为什么ConcurrentHashMap放弃了分段锁设计？

jdk8 放弃了分段锁而是用了Node锁，减低锁的粒度，提高性能，并使用CAS操作来确保Node的一些操作的原子性，取代了锁。

#### ArrayBlockingQueue的底层实现？ //TO-DO

ArrayBlockingQueue是数组实现的线程安全的有界的阻塞队列，继承自AbstractBlockingQueue,间接的实现了Queue接口和Collection接口。底层以数组的形式保存数据(实际上可看作一个循环数组)。



### 线程池相关 //TO-DO: 不熟

#### 线程池

线程使应用能够更加充分合理地协调利用 CPU、内存、网络、 1/0 等系 统资源。 结程的创建需要开辟虚拟机枪、本地方法枝、程序计数器等线程私有的内存空间。在 线程销毁时需要回收这些系统资源。频繁地创建和销毁线程会浪费大量的系统资源， 增加并发编程风险。另外，在服务器负载过大的时候，如何让新的线程等待或者友好 地拒绝服务?这些都是线程自身无法解决的。所以需要通过线程池协调多个线程 ， 并 实现类似主次线程隔离、定时执行、周期执行等任务。线程池的作用包括：

- 管理复用线程，控制最大并发数
- 实现任务线程队列缓存策略和拒绝机制
- 实现某些与定时相关的功能，如延迟执行，周期执行
- 隔离线程环境

#### ThreadPoolExecutor

**构造函数参数**：

- corePoolSize：常驻核心线程数。如果等于0，没有任何请求进入时销毁线程池的线程；如果大于0，不管有没有任务在执行，这些线程也不会被销毁。
- maximumPoolSize：线程池最大线程数。必须大于等于1，如果与corePoolSize相等，则是维护固定数量的线程。
- keepAliveTime：表示线程空闲时间，当空闲时间达到keepAliveTime时，线程就会被销毁，只保留corePoolSize个线程；当allowCoreThreadTimeOut设置为true时，核心线程亦会被销毁。
- workQueue：缓存队列。当请求的线程数大于 maximumPoolSize 时 ， 线程进入 BlockingQueue 阻塞队列。
- threadFactory：表示线程工厂。用来生产一组相同任务的线程。
- handler：表示执行拒绝策略的对象。当超过参数 workQueue 的任务缓存区上限的时候，就可以通过该策略处理请求，这是种简单的限流保护。

**ThreadPoolExecutor中提供了四个公开的静态类**：

- AbortPolicy：默认策略，表示丢弃任务并抛出异常
- DiscardPolicy：丢弃任务但是不抛出异常
- DiscardOldestPolicy：丢弃队列中等待最久的任务，然后把新任务添加到队列里,不会抛异常。
- CallerRunsPolicy：调用任务的run方法绕过线程池直接执行

发生拒绝的原因有两个：1）线程池状态为非RUNNING状态 	2）等待队列已满

***

**线程池创建线程策略**：

如果运行的线程少于 corePoolSize，则 Executor始终首选添加新的线程，而不进行排队。（如果当前运行的线程小于corePoolSize，则任务根本不会存放，添加到queue中，而是直接抄家伙（thread）开始运行）

如果运行的线程等于或多于 corePoolSize，则 Executor始终首选将请求加入队列，**而不添加新的线程**。

如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。

***

**排队的策略**：

- **直接提交。**工作队列的默认选项是 SynchronousQueue，它将任务直接提交给线程而不保持它们。在此，如果不存在可用于立即运行任务的线程，则试图把任务加入队列将失败，因此会构造一个新的线程。此策略可以避免在处理可能具有内部依赖性的请求集时出现锁。直接提交通常要求无界 maximumPoolSizes 以避免拒绝新提交的任务。当命令以超过队列所能处理的平均数连续到达时，此策略允许无界线程具有增长的可能性。

- **无界队列。**使用无界队列（例如，不具有预定义容量的 LinkedBlockingQueue）将导致在所有corePoolSize 线程都忙时新任务在队列中等待。这样，创建的线程就不会超过 corePoolSize。（因此，maximumPoolSize的值也就无效了。）当每个任务完全独立于其他任务，即任务执行互不影响时，适合于使用无界队列；例如，在 Web页服务器中。这种排队可用于处理瞬态突发请求，当命令以超过队列所能处理的平均数连续到达时，此策略允许无界线程具有增长的可能性。

- **有界队列。**当使用有限的 maximumPoolSizes时，有界队列（如 ArrayBlockingQueue）有助于防止资源耗尽，但是可能较难调整和控制。队列大小和最大池大小可能需要相互折衷：使用大型队列和小型池可以最大限度地降低 CPU 使用率、操作系统资源和上下文切换开销，但是可能导致人工降低吞吐量。如果任务频繁阻塞（例如，如果它们是 I/O边界），则系统可能为超过您许可的更多线程安排时间。使用小型队列通常要求较大的池大小，CPU使用率较高，但是可能遇到不可接受的调度开销，这样也会降低吞吐量。 

#### runnable和callable

- runnable没有返回值，而callable有返回值，和Future/FutureTask可以异步获取执行结果。

多线程返回执行结果是很有用的一个特性，因为多线程相比单线程更难、更复杂的一个重要原因就是因为多线程充满着未知性，某条线程是否执行了？某条线程执行了多久？某条线程执行的时候我们期望的数据是否已经赋值完毕？无法得知，我们能做的只是等待这条多线程的任务执行完毕而已。而Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下**取消**该线程的任务，真的是非常有用。

#### BlockingQueue

| 方法 | 抛出异常  | 特殊值   | 阻塞   | 超时                 |
| ---- | --------- | -------- | ------ | -------------------- |
| 插入 | add(e)    | offer(e) | put(e) | offer(e, time, unit) |
| 移除 | remove()  | poll()   | take() | poll(time, unit)     |
| 检查 | element() | peek()   |        |                      |

```BlockingQueue```是一个接口。

#### 线程池阻塞队列 //TO-DO

- ArrayBlockingQueue
- LinkedBlockingQueue
- DelayQueue
- PriorityBlockingQueue
- SynchronousQueue

**ArrayBlockingQueue：** （有界队列）是一个用数组实现的有界阻塞队列，按FIFO排序量。

**LinkedBlockingQueue：** （可设置容量队列）基于链表结构的阻塞队列，按FIFO排序任务，容量可以选择进行设置，不设置的话，将是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE，吞吐量通常要高于ArrayBlockingQuene；newFixedThreadPool线程池使用了这个队列

**DelayQueue：**（延迟队列）是一个任务定时周期的延迟执行的队列。根据指定的执行时间从小到大排序，否则根据插入到队列的先后排序。newScheduledThreadPool线程池使用了这个队列。

**PriorityBlockingQueue：**（优先级队列）是具有优先级的无界阻塞队列；

**SynchronousQueue：**（同步队列）一个不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQuene，newCachedThreadPool线程池使用了这个队列。针对面试题：线程池都有哪几种工作队列？

我觉得，回答以上几种ArrayBlockingQueue，LinkedBlockingQueue，SynchronousQueue等，说出它们的特点，并结合使用到对应队列的常用线程池(如newFixedThreadPool线程池使用LinkedBlockingQueue)，进行展开阐述， 就可以啦。

#### 线程池怎么知道线程已经将任务执行完毕了？

- 线程池内部，如果run()方法执行结束，那么任务执行完毕
- submit()方法执行任务，返回Future\<T>，可以通过future.get()方法来获取返回值。 

## JVM

### 垃圾收集算法

- 分代收集理论：JVM应该将堆划分出不同的区域，然后将需要回收的对象按照其年龄分配到不同的区域中进行存储。
- 内存一般至少分为新生代和老年代两部分。
- 标记-清除算法：
  - 算法分为标记和清除两个阶段。首先标记需要回收的对象，在标记完成后统一回收掉所有被标记的对象。也可以反过来标记存活的对象，最后回收未被标记的对象。
  - 缺点是 1）. 运行效率不稳定，如果有大量对象需要回收，那么就必须进行大量的标记和清除的工作。 2）会产生内存碎片。
- 标记-复制算法：
  - 将内存分区，每次只使用其中一块，当GC时，将还存活的对象复制到另一块内存上，然后把原来的内存区域完全清理掉。
  - 缺点是会产生大量的内存间复制的开销，而且会浪费一部分内存。
  - 有些商用的虚拟机会将新生代分为一块大的Eden区域和两块小的Survivor空间，每次只使用Eden和Survivor中的一块。GC时将Eden和使用中的Survivor中的存活对象复制到另一块Survivor中。当原来未使用的Survivor中内存不够时，就会使用老年代（old gen）内存进行担保分配。
- 标记-整理算法：
  - 标记对象，然后将存活的对象移动到内存的一端，然后将其余区域的所有内存都清除。



- GC Roots
  - 虚拟机栈里的本地变量引用
  - 方法区静态属性引用
  - 方法区常量池引用
  - JNI中的引用



- 为什么不在新生代采用标记-清除算法：
  - 从内存连续性考虑，新生代发生GC的频率相对来说是比较高的，如果采用标记-清除算法的话，新生代经历多轮GC就会产生很多的内存碎片，导致虽然有足够的内存空间却无法分配给对象。
  - 从效率来考虑，如果新生代存活的对象比较少，那么清除过程就会比较耗时，效率比较低
- 为什么新生代不能采用标记-整理算法：
  - 我的理解是可以，但没有必要，首先标记-整理算法的复杂度肯定是要高于标记-复制的，因为标记-整理算法除了标记外还涉及到对象位置的移动。新生代使用标记-复制算法需要舍弃一小部分可用内存，但是可以换取来效率的提升。
- 为什么在老年代不采用标记-复制算法：
  - 我的理解是，老年代的对象一般来说是不大可能被GC掉的，就是说一次GC后老年代对象的存活率是比较高的，这就意味着如果我们如果在老年代采用标记-复制算法的话，就需要提前准备好一块大内存做备用，相比于新生代浪费的空间太大了，因此不适合在老年代使用。

### 如何用写代码的方式触发young gc、full gc

- young gc
  - 
- full gc发生原因
  - System.gc()
  - 老年代空间不足
  - 方法区空间不足
  - 空间担保机制
  - 直接在堆中分配很大的对象

### 垃圾收集器

![image-20220325161249230](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/java_gc_collectors.png)

#### 引用计数法为什么无法回收循环依赖的对象

就算外部没有对循环引用对象的引用了，循环依赖对象的引用计数也不为0，导致无法回收。



#### Serial收集器

- 新生代收集器
- 采用标记-复制算法
- 大量过程需要STW，适合CPU核心数较少，Java管理的堆内存较小的场景，因此比较适合做客户端的垃圾收集器。
- 关注点是尽量缩短垃圾收集时用户线程的停顿时间。

#### ParNew

- 实际上是Serial收集器的多线程版本，即在标记复制时使用多线程完成。

#### Parallel Scavenge

- 新生代收集器
- 采用标记-复制算法
- 关注点在吞吐量：运行用户代码的时间与处理器总消耗时间的比值。

#### Serial Old

- Serial收集器的老年代版本
- 单线程
- 采用标记-整理算法

#### Parallel Old

- Parallel Scavenge的老年代版本
- 采用标记-整理算法
- 吞吐量优先

#### CMS（Concurrent Mark Sweep）

- 以获取最短回收停顿时间为目标的收集器
- 采用标记-清除算法
- 标记阶段复杂
  - 初始标记：STW，标记GC Roots能直接关联的对象
  - 并发标记
  - 重新标记：STW
  - 并发清除
- 缺点：
  - 对资源敏感，在并发阶段，虽然不会导致用户线程暂停，但是却因为占用了一部分资源而导致程序变慢，吞吐量变低。
  - 无法处理”浮动垃圾“（并发时用户线程产生的垃圾，只能留到下次再GC），有可能导致Concurrent Mode Failure而引发一次完全STW的Full GC。
  - 同样因为用户线程也在运行，因此CMS要预留空间给用户线程使用，不能等老年代几乎快被填满了再进行收集，必须预留一部分空间供并发收集时的程序运作使用。如果CMS GC期间预留的内存不够分配使用，会引发并发失败，导致一次Full GC产生。
  - 基于标记-清除，会有内存碎片产生。

#### Garbage First（G1）垃圾收集器

- G1内存布局不再像传统的垃圾收集器一样，在堆内存管理方面将内存逻辑上直接划分为新生代和老年代两大块（并不是说G1不遵循分代收集理论），而是采用了基于Region的内存布局形式以及收集器面向局部收集的收集形式。

- 主要面向服务端的GC
- 垃圾收集的衡量标准不再是内存属于哪个分代，而是哪块内存中存放的垃圾最多，回收收益最大。
- 将内存划分为多个大小相等的Region，每一个Region都可以根据需要去扮演Eden、Survivor、老年空间。G1能对扮演不同角色的Region采用不同的策略去处理。此外还有Humongous区域，专门来存储大对象。
- G1之所以可以建立可预测的停顿时间模型，是因为它将Region作为单次回收的最小单元，即每次回收内存大小都是Region的整数倍。
- G1至少要需要堆总容量的10%~20%的内存来维持收集器工作。
- 采用标记-回收的形式进行GC（整体上看像是标记整理，局部即两个Region之间看像是标记复制）。
- 默认的目标停顿时间是200ms，不能设置的太低，设置的太低会导致收集速度跟不上垃圾产生速度而最终触发FullGC。
- G1回收过程
  - 初始标记：STW
  - 并发标记
  - 最终标记：STW
  - 筛选回收：STW

#### ZGC

- 整个垃圾收集过程几乎全是并发的，只有初始标记、最终标记有停顿，这部分停顿时间是固定的，跟堆大小、堆中对象的数量没有正比关系。
- GC停顿控制在10ms以内。
- 与Azul PGC和C4 GC只有术语上的差异
- 基于Region内存布局
- 使用读屏障、染色指针和内存多重映射
- 可与用户线程并发的标记-整理算法
- 染色指针是一种将少量额外信息存储在指针上的技术。通过着色指针就可以知道引用对象的三色标记状态、是否进入重分配集、是否只能通过finalize()方法才能访问到。
- 染色指针使得一旦某个Region中的存活对象被移走之后，这个Region能立即被释放和重用掉，而不必等待整个堆中所有指向该Region的引用被修正后才能清理。
- 多重映射：几个虚地址映射到同一个物理地址

![image-20220326221117078](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/multi_map_memory.png)

ZGC回收阶段;

- 并发标记：遍历对象图进行可达性分析，对指针进行染色，会更新染色指针上的marked0、marked1标志。
- 并发预备重分配：
- 

#### SGC

- 整个垃圾收集过程几乎全是并发的，只有初始标记、最终标记有停顿，这部分停顿时间是固定的，跟堆大小、堆中对象的数量没有正比关系。

- GC停顿控制在10ms以内。

- 虽然在内存布局、默认回收策略方面与G1十分相似，但是它与G1至少有三点不同

  - 最重要的是支持并发的整理算法，G1虽然整理阶段是多线程的，但是不能与用户线程并发。
  - SGC在Open JDK12 暂时不支持分代回收
  - 抛弃了G1中的记忆集，改用名为”连接矩阵“的全局数据结构来记录跨Region的引用关系，降低了处理跨代指针时的记忆集维护消耗，也降低了伪共享的发生概率。

- GC过程

  - 初始标记：STW
  - **并发标记：**遍历对象图，标记出全部可达的对象，这个阶段是与用户线程一起并发的，时间长短取决于堆中存活的对象数量以及对象图结构的复杂度。
  - 最终标记：统计出回收价值最高的Region，组成回收集。会STW。
  - 并发清理：处理没有任何存活对象的Region。
  - **并发回收：**区别与G1的核心部分，SGC需要把存活的对象先复制一份到其它未被使用的Region之中。复制对象这件事情如果将用户线程冻结的话是比较好做的，但是如果要并行的话就难起来了。难点在于移动对象的同时，用户线程仍然可能不停对被移动的对象进行读写访问。移动对象是一次性行为，但是对于整个内存中对于该对象的引用还是旧对象的地址，这是很难一瞬间全改过来的。对此，SGC通过读屏障和”Brooks Pointers“的转发指针来解决。
  - 初始引用更新：并发回收阶段复制完对象后，还需要修正所有旧的引用。该阶段不会更新旧的引用，而是建立一个线程集合点，确保所有的对象已经被复制。该阶段会有短暂的停顿。
  - **并发引用更新**：真正开始引用更新工作，这个阶段是与用户线程并发的。并发更新与并发标记阶段不同，该阶段不再需要沿着对象图搜索，只需要按照物理内存顺序，线性的搜索出引用类型，然后把旧值改为新值。
  - 最终引用更新：解决了堆中的引用更新后，还要修正存在于GC Roots 中的引用。这个阶段是Shenandoah的最后一次停顿，停顿时间只与GC Roots的数量相关。
  - 并发清理：经过并发回收和引用更新之后，整个回收集中所有的Region已 再无存活对象，这些Region都变成Immediate Garbage Regions了，最后再调用一次并发清理过程来回收 这些Region的内存空间，供以后新对象分配使用。

- Brooks Pointers

  在此之前，要做类似的并发操作，通常是在被移动的对象的原有内存上设置保护陷阱，一旦用户程序访问旧的对象，就会陷入陷阱，进入预设好的异常处理器中，再由其中的代码逻辑把访问转发到新对象上。但是这种方案如果没有操作系统层面的直接支持，将会导致用户态频繁切换到内核态。

  Brooks提出的新方案不再使用保护陷阱，而是在原有对象布局结构前统一增加一个新的引用字段，在正常不处于并发移动的情况下，该引用指向对象自己。这样虽然会导致访问对象时多了一次转发开销（相比于保护陷阱好多了），好处是对象移动时只需要修改这一个值，在旧的对象未被清理之前，虚拟机内存中所有对旧的对象的操作会全部转发到新的对象上。

  Brooks Pointers是通过CAS和读写平展来保证并发时操作正确性的。读屏障带来的性能损失是要大于写屏障的，因为读的发生次数要比写的发生次数多的多，读屏障的使用量也要大于写屏障。

- 高负载下，SGC会发生吞吐量下降的情况。

#### 衡量GC优劣的指标

- 额外内存占用
- 吞吐量
- 延迟

随着硬件的更新迭代，对额外内存占用（内存变大）和吞吐量（CPU性能、核心数）的要求没有以前那么迫切了，因此新的GC优化方向都是朝着低延迟（内存变大反而会导致GC延迟变高，在1G的堆上进行垃圾回收肯定要比在1T的堆上进行要快）进行。

![image-20220325204821969](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/all_gc_state.png)

### 实战

#### 对象优先在Eden区分配

大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。

#### 大对象直接进入老年代

大对象就是指需要大量连续内存空间的Java对象，最典型的大对象便是那种很长的字符串，或者 元素数量很庞大的数组，本节例子中的by t e[]数组就是典型的大对象。大对象对虚拟机的内存分配来说 就是一个不折不扣的坏消息，比遇到一个大对象更加坏的消息就是遇到一群“朝生夕灭”的“短命大对 象”，我们写程序的时候应注意避免。在Java虚拟机中要避免大对象的原因是，在分配空间时，它容易 导致内存明明还有不少空间时就提前触发垃圾收集，以获取足够的连续空间才能安置好它们，而当复 制对象时，大对象就意味着高额的内存复制开销。

#### 长期存活的对象进入老年代

HotSpot虚拟机中多数收集器都采用了分代收集来管理堆内存，那内存回收时就必须能决策哪些存 活对象应当放在新生代，哪些存活对象放在老年代中。为做到这点，虚拟机给每个对象定义了一个对 象年龄(Age)计数器，存储在对象头中(详见第2章)。对象通常在Eden区里诞生，如果经过第一次 Minor GC后仍然存活，并且能被Survivor容纳的话，该对象会被移动到Survivor空间中，并且将其对象 年龄设为1岁。对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程 度(默认为15)，就会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold设置。



#### jps：虚拟机进程状况工具

java版的ps命令。

- -q：只输出LVMID，忽略主类的名称
- -m：输出传递给main()函数的参数
- -l：输出主类的全名，如果进程执行的是Jar包，则输出jar包路径
- -v：输出虚拟机启动时的JVM参数。

#### jstat：虚拟机信息统计工具

jstat(JVM Statistics M onitoring Tool)是用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或者远程[1]虚拟机进程中的类加载、内存、垃圾收集、即时编译等运行时数据，在没有 GUI图形界面、只提供了纯文本控制台环境的服务器上，它将是运行期定位虚拟机性能问题的常用工具。

jstat命令格式：

```bash
jstat [option vmid [interval[m|ms]] [count]]
```

- vmid：jps显示的id号

- interval：查询间隔

- count：查询次数

- option：

  ![image-20220326230525707](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/jstat_options.png)

#### jinfo：Java配置信息工具



### JVM调优

#### 为什么要调优

- 比如对于年轻代的Eden区，这块区域主要是去负责新生对象的内存分配，如果太小的话就会频繁的出发YoungGC，太大的话又会导致YoungGC时间变长。
- Survivor区太小，GC时可能要让老年代做分配担保，会直接让对象进入老年代。而Survivor太大的话，会浪费内存，因为ToSurvivor区是不用的，只在复制过程使用。
- 如果区域还有很多的空闲空间却频繁GC，可能是因为内存碎片话导致的无法分配。主要会发生在老年代和方法区。

#### 怎么判断JVM频繁GC？

- jps查找vmid
- jstat 查看GC情况

#### 怎么排查JVM内存泄漏

- 用jmap等工具dump转储java堆区
- 根据dump文件，先查找可能内存泄漏的类对象，定位到以后看引用链情况。

#### JVM频繁GC怎么去优化

- 首先得找出频繁GC的原因，我们就比如说频繁YounGC，这个原因可能就是新生代Eden区分配的太小了，这种情况直接增加Eden区大小就行了；如果Eden区已经很大了还是频繁GC，这个时候应该要去排查代码了，是不是有对象频繁分配。
- 如果频繁FullGC，这个情况比较复杂，但归根结底来说就是老年代空间不足了，造成老年代空间不足的原因还是有很多的，比如有超大对象分配，超大对象是直接进老年代的，又或者新生代频繁创建对象，频繁YoungGC导致有很多对象存活到老年代或者ToSurvivor区不够分让老年代进行分配担保了，又或者发生了内存泄漏，老年代主流了大量释放不掉的对象。

# Redis

## 项目为什么用Redis？

在项目中使用Redis主要考虑性能和并发，如果只是为了实现分布式锁的话，可以用Zookeeper等中间件，并非一定要用Redis。

- 性能

  我们在碰到需要执行耗时特别久，且结果不频繁变动的 SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应。

  特别是在秒杀系统，在同一时间，几乎所有人都在点，都在下单。。。执行的是同一操作———向数据库查数据。

- 并发

  在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用 Redis 做一个缓冲操作，让请求先访问到 Redis，而不是直接访问数据库。

## Redis基础类型

- String
  - 最常规的 set/get 操作，Value 可以是 String 也可以是数字。一般做一些复杂的计数功能的缓存。
- Hash
  - 这里 Value 存放的是结构化的对象，比较方便的就是操作其中的某个字段。我在做单点登录的时候，就是用这种数据结构存储用户信息，以 CookieId 作为 Key，设置 30 分钟为缓存过期时间，能很好的模拟出类似 Session 的效果。
- List
  - 使用 List 的数据结构，可以做简单的消息队列的功能。另外，可以利用 lrange 命令，做基于 Redis 的分页功能，性能极佳，用户体验好。
- Set
  - 因为 Set 堆放的是一堆不重复值的集合。所以可以做全局去重的功能。我们的系统一般都是集群部署，使用 JVM 自带的 Set 比较麻烦。另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。
- Sorted Set（ZSet）
  - Sorted Set 多了一个权重参数 Score，集合中的元素能够按 Score 进行排列。可以做排行榜应用，取 TOP N 操作。Sorted Set 可以用来做延时任务。



## 跳表  //TO-DO:

## Redis缓存雪崩、缓存击穿、缓存穿透

- **缓存雪崩**

  当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。

  **<u>解决方案：</u>**

  1）熔断：如果某个服务调用慢或者有大量超时的情况，就熔断该服务的调用，不再继续提供服务，直到情况缓解。

  2）隔离模式：可以对不同类型的请求使用线程池来资源隔离，每种类型的请求互不影响，如果一种类型的请求线程资源耗尽，则对后续的该类型请求直接返回，不再调用后续资源。

  3）限流模式：熔断模式和隔离模式都属于出错后的容错处理机制，而限流模式则可以称为预防模式。限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。

  4）不同的key设置不同的过期时间

- **缓存击穿**

  key对应的数据存在，但在redis中过期，如果我们存的是热点数据，那么此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。

  <u>**解决方案：**</u>

  对于产生缓存击穿是数据，一般是很热点的数据才会考虑缓存被击穿的情形。解决的方案一般是使用互斥锁（mutex lock）。就是说如果对于某个热点数据没有，是不会立即去DB load数据的，而是先尝试获取一个锁（mutex），成功获取锁的线程去查询db load，没有成功获取锁的线程，尝试继续get。

  ```java
  if (redis.setnx(key_mutex)) {
    	value = db.get(key);
    	redis.set(key, value);
    	redis.del(key_mutex);
  } else {
    	sleep(50);
    	get(key);
  }
  ```

- **缓存穿透**

  key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。

  **<u>解决方案：</u>**

  1）采用布隆过滤器。布隆过滤器可能的问题是把白名单中的某个数据误判成黑名单里的数据，但是对于黑名单里的数据，一定是可以确认的。创建布隆过滤器时的所有数据，查询得到的结果一定是存在，但是对于某些不存在的值，也有可能判定为存在。布隆过滤器会告诉你某些key**一定不存在**（判断不存在的一定不存在）和**可能存在**（判定存在的，只是可能存在）。

  2）对于通过了布隆过滤器但是查询仍不存在的数据，直接存到redis里，然后设置一个比较短的过期时间。

# Linux

[linux常用命令](https://www.cnblogs.com/xuxinstyle/p/9609551.html)

查看硬件信息相关命令就有8个，分别是ifconfig、free、fdisk、ethtool、mii-tool、dmidecode、dmesg、lspci，用于系统性能监视高级命令有uptime、top、free、vmstat、mpstat、iostat、sar、chkconfig，用于内存的命令是top、free、vmstat、mpstat、iostat、sar等。

## 查看进程并杀死进程

```bash
ps -ef | grep "redis"
kill -s 9 pid
```

## 查找文件

```bash
find path expression
# 按文件名
find / -name ""
```

## grep

用于查找文件里符合条件的字符串。

```bash
grep expression
```

```grep -v grep```去除grep进程,防止影响结果的准确性。

## top

linux中任务管理器

## ps -ef

ps：默认显示属于当前用户的进程

-e：显示所有进程

-f：扩展显示输出

## xargs

xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。

## awk

awk其实是一门编程语言，它支持条件判断、数组、循环等功能。所以，我们也可以把awk理解成一个脚本语言解释器。

```bash
awk [options] 'Pattern{Action}' file
```

awk '{print $2}' $fileName : 一行一行的读取指定的文件， 以空格作为分隔符，打印第二个字段

```basha
ps -ef|grep nginx|grep -v grep|awk '{print $2}'|xargs kill -9
```

[批量杀死进程](https://blog.csdn.net/weixin_37460672/article/details/102855911)



## 如何由用户态陷入内核态？

x86架构下通过```int 80H```指令，x64架构下通过```syscall```指令

# 数据结构&算法

## 栈

### 最小栈

1）两个栈实现，一个栈保存压入的值，另一个栈同步压入当前的最小值，弹出时同步弹出，这样最小值栈的栈顶永远是当前的最小值。栈中压入abcd，d弹出之前，abc不可能弹出。

2）保存差值，栈中保存当前入栈元素与当前最小值的差值。假设栈中压入abcde， abcde中最小值是c，那么栈中存储的“d”，“e”一定是一个正值或者是0（说明压入了和当时最小值一样大小的值），弹出时，只需要加上curMin即可还原原来的值。而出栈时如果遇到了负数，说明这里是曾经改变了最小值的地方，也就是说这个负数就是当前最小值比前一个最小值小的数值，那么在弹出这个数时就需要更新当前最小值。

第二种方法对数值有限制，计算差值不能溢出才行。

3）使用自定义链表，思想类似 1）

### 逆转栈，不允许使用额外空间

## 海量数据相关 

### 一致性Hash

#### murmurhash

### hash函数均匀映射

### 布隆过滤器

### 位图

### 分段

### 利用堆、外排序来做多个处理单元的结果合并

# 面试问题记录

## 美团

### 5G是哪一层的

### MySQL 银行转账用哪个事务隔离级别

### redis登陆



# 面经/面试相关

## 常见排序算法时间复杂度

![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/sort_time.png)

选择排序算法的依据：



- 冒泡排序
  - 从左向右两两比较，大的交换到右边，因此有序序列是从右侧开始往左侧扩张的
  - 一样大小的值不会交换顺序，故稳定。
  - **一般用于学习，实际应用没每听说过哪里用冒泡排序了。**
- 选择排序
  - 左边序列为已排序序列。
  - 从右侧未排序的序列中找最小值，将最小值交换到已排序序列的尾部，已排序序列大小+1；
  - 交换的时候有可能把某个等值元素的位置移动到了另一个等值元素的后面了，故不稳定。
  - 可以稳定，但没必要。需要将无序部分交换位置前的所有元素右移一位。
  - **当数据量较少且不要求稳定性的时候使用。**
- 插入排序
  - 左边序列为已排序序列。
  - 对于元素i，在左边已排序序列中找到不大于自己的元素，将自己排到它后面。
  - 对于值相同的元素，不会排到它前面（闲的吗这不是？），因此该算法稳定。
  - **当数据距离它的正确位置很近或者近乎有序的情况下使用插入排序，或者元素个数少，要求稳定性时使用**。
- 希尔排序
  - 先选择一个步长gap，将数组分成gap个组。gap = length/2；
  - 在每个子数组上进行插入排序。然后gap/=2；
  - 当gap=1时相当于对数组排序。
  - 因为分组了，不能保证原来一样的值排序后在整个数组中还是相对位置不变的，故不稳定。
  - **相对于直接插入排序，希尔排序要高效很多，因为当gap 值较大时，对子数组进行插入排序时要移动的元素很少，元素移动的距离很大，这样效率很高；在gap逐渐减小过程中，数组中元素已逐渐接近排序的状态，所以需要移动的元素逐渐减少；当gap为1时，相当于进行一次直接插入排序，但是各元素已接近排序状态，需要移动的元素很少且移动的距离都很小。**
- 归并排序
  - 基于分治的想法
  - 稳定
- 快速排序
  - 分治的思想，每次选择一个pivot作为基准，大于等于pivot的全部挪到右边，小于等于pivot的全部一定到pivot左边
  - partition算法中，如果两个数大小相等也要交换顺序，否则会死循环。故不稳定。
  - **元素个数多，初始元素分布随机，稳定性不做要求的情况下，选择使用快速排序。**
- 堆排序
  - 利用siftDown操作，首先从最后一个有儿子的节点开始siftDown，直到堆顶。
  - 每次将堆顶与堆尾元素交换，相当于删除了堆顶元素，然后对堆顶siftDown。重复直到只剩一个元素。
  - **堆排序适合数据量非常大的场合。堆排序不需要大量的递归或者额外的暂存数组。**
- 计数排数
  - 用数组统计每个数出现的次数
  - 适用于数据比较集中的情况。
- 桶排序
  - 
- 基数排序



[对线面试官](http://javainterview.gitee.io/luffy/)

# 服务器性能测试工具

## Apache JMeter





# 分布式秒杀系统

[浅谈秒杀系统](https://blog.csdn.net/duan196_118/article/details/105206392)

[秒杀系统设计4要素：硬抗高并发，拒绝超卖，避免少卖，打击黄牛](https://zhuanlan.zhihu.com/p/429007941)

- 硬抗高并发
  - 我们可以先将库存名额预加载到Redis，然后在Redis中进行扣减，扣减成功的再通过消息队列，传递到MySQL做真正的订单生成。
  - 为什么要通过消息队列呢？**主要有两点好处，一个是这种投递的方式，可以让抢和购解耦。另一个是可以很方便地限频，不至于让MySQL过度承压。**
  - 说回Redis，如果请求量超过6W每秒，就要考虑使用多个Redis来分流。预计有100W请求量，我们就可以临时调度20个Redis实例来支持，一个5W/s，留点Buffer。这种模式倒是不需要使用Redis Cluster那种一致性Hash的做法，直接前面接个Nginx，做负载均衡就可以了。
- 拒绝超卖
  - 抢购场景最核心的两个步骤：①判断库存是否充足；②减少库存名额，扣减成功就是抢到。这两个操作都需要是原子操作。
  - 解决方案1：lua脚本，在lua脚本中调用多个Redis命令，这些命令整体上会作为原子操作来进行。
  - 解决方案2：Zookeeper分布式锁或者Redis分布式锁
  - 单机部署解决方案：直接使用语言自带的同步手段即可。
- 避免少卖
  - 什么时候会出现少卖呢？库存减少了（Redis），订单却没有生成。在Redis操作成功，但是向Kafka发送消息失败，这种情况就会白白消耗Redis中的库存。
  - 作为一个专业的程序员，只要知道问题是什么、怎么发生的，问题就解决了一半。说白了，我们只需要**保证Redis库存+Kafka消耗的最终一致性**。
  - 解决方案1：在投递Kafka失败的情况下，增加渐进式重试；
  - 解决方案2：就是在第一种的基础上，将这条消息记录在磁盘上，慢慢重试；
- 打击黄牛：
  - 限购，将逻辑写入lua脚本，**优化为第一步查询库存，第二步查询用户已购买个数，第三步扣减库存，第四步记录用户购买数**。



[服务熔断与降级](https://www.cnblogs.com/rjzheng/p/10340176.html)

- 服务雪崩：
  - A服务调用B服务，B服务又调用C服务，如果A服务有很大的并发量，假设A服务能抗的住，但B和C服务未被能抗的住。如果某个时刻服务C顶不住压力挂掉了，那么B对C的请求就会阻塞，慢慢的堆积的阻塞线程多了B也会不可以，最终导致整个系统的某个调用链上的服务全部宕机。

- 服务熔断：

  - 当下游的服务因为某些原因变得响应过慢或者不响应的话，上游服务为了保证自身整体服务的可用性，不再调用下游服务，而是直接返回，快速释放资源。如果目标服务情况好转则恢复调用。

  - 需要说明的是熔断其实是一个框架级的处理，那么这套熔断机制的设计，基本上业内用的是`断路器模式`，如`Martin Fowler`提供的状态转换图如下所示

    ![img](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/server_fuse.png)

- 服务降级：

  - 当下游的服务因为某种原因**响应过慢**，下游服务主动停掉一些不太重要的业务，释放出服务器资源，增加响应速度！
  - 当下游的服务因为某种原因**不可用**，上游主动调用本地的一些降级逻辑，避免卡顿，迅速返回给用户！
  - 是从整个系统的负载情况考虑的，当整个系统的负载比较高时，先暂停一些非核心接口的服务，直接返回一个fallback错误信息。这样虽然提供的是一个有损的服务，但是却保证了整个系统的可用性。

![输入图片说明](https://haiqiang-picture.oss-cn-beijing.aliyuncs.com/blog/184617_c7e13059_87650.png)

- 架构及思路
  - 防止DDos恶意攻击，使用高仿IP。
  - 进行SLB，对多台云服务器进行流量转发。
  - 使用Nginx进行限流分发
  - 后端秒杀逻辑：基于Redis或者zookeeper分布式锁，Kafka或者Redis做消息队列，DRDS中间件实现数据库的读写分离
- 优化思路：
  - 分流、分流、分流，重要的事情说三遍，再牛逼的机器也抵挡不住高级别的并发。
  - 限流、限流、限流，毕竟秒杀商品有限，防刷的前提下没有绝对的公平，根据每个服务的负载能力，设定流量极限。
  - 缓存、缓存、缓存、尽量不要让大量请求穿透到DB层，活动开始前商品信息可以推送至分布式缓存。
  - 异步、异步、异步，分析并识别出可以异步处理的逻辑，比如日志，缩短系统响应时间。
  - 主备、主备、主备，如果有条件做好主备容灾方案也是非常有必要的(参考某年锤子的活动被攻击)。
  - 最后，为了支撑更高的并发，追求更好的性能，可以对服务器的部署模型进行优化，部分请求走正常的秒杀流程，部分请求直接返回秒杀失败，缺点是开发部署时需要维护两套逻辑。



- 分流：Nginx分流
  - ip轮询：不考虑服务器的处理能力，每个请求按时间逐一分配到不同的后端服务器，如果服务器down掉，自动剔除。
  - 权重策略：根据每台机器的处理能力来分配请求。
  - ip_hash
- DNS负载均衡先，先让域名绑定多个ip，每个ip分配给一个nginx分发服务器。
- 每台nginx做主从，用vip，主机down掉后刷新路由器的arp映射。怎么知道主机down掉了？心跳包。
- 限流：
  - 基于连接：限制最大连接数；基于请求：限制访问频率。
  - Nginx令牌桶：以固定的速度往令牌桶放令牌，最大n个。每个请求消耗一个令牌，没有令牌时拒绝请求。允许突发请求。
  - 漏桶：一个固定容量的漏桶，按照固定速率流出水滴。遇到突发请求，也是按照一定的速率处理。
  - 消息队列削峰填谷
- 缓存：
  - 使用redis防止请求大量穿透到DB层。





- 如何防止单个用户重复秒杀下单？
- 如何防止恶意调用秒杀接口？
- 如果用户秒杀成功，一直不支付该怎么办？
- 消息队列处理完成后，如果异步通知给用户秒杀成功？
- 如何保障 Redis、Zookeeper 、Kafka 服务的正常运行(高可用)？
- 高并发下秒杀业务如何做到不影响其他业务(隔离性)？



# 消息队列

## 消息队列的作用

- 解耦：作为中间件，实现两个系统之间的解耦。
- 异步：系统A将信息发送给消息队列后，就可以去做其他事情了。
- 流量削峰：将调用请求放到消息队列中，消息在消息队中排队，由消费者按照一定的速度消费。

# 深度学习项目

### 项目整体流程：

- 训练模型

  - 预测模型
  - 诊断模型
  - 使用过的模型
    - 全连接模型：BaseLine
    - CNN：
      - 卷积步长的选择
      - 有通道
    - RNN：
      - Simple-RNN：只传递一个状态h^t，计算t时刻状态毅来t-1时间的h^t。它只能通过叠加的方式来记忆，所有会有忘事的问题，即只能记忆短时间的信息。
      - LSTM：除了传递状态h^t，还传递c^t细胞状态。h^t在不同的节点往往会有很大区别，而c^t变化的很慢，这个主要是通过各种门控状态来实现的。有三个门：forget、input、output。forget来控制上一时刻c的哪些信息需要保留哪些信息需要遗忘，input门的话会对当前时刻的信息进行选择记忆，只记录重要的。最后output门输出当前时刻的c。
      - GRU：同样只传递h^t。只有两个门，update和reset。update门定义了之前的记忆保存到当前时间步的量，reset门决定了如何将新的输入信息与前面的记忆相结合。
    - ResNet
      - 通过恒等连接，在一定程度上解决了梯度爆炸和梯度消失的问题。
    - Transformer
      - 只使用了注意力机制，没有采用卷积等操作
      - 多头注意力机制-类似CNN通道
      - 相比RNN，Transformer可以更好的并行训练
      - 同样使用了ResNet的恒等连接

   

  - 遇到的问题
    - 数据不均衡
      - 参考了工业与信息融合这篇顶刊上的论文《[Deep Feature Generating Network](http://ieeexplore.ieee.org/document/9224196)》来解决的。期刊影响因子8.几
      - 这篇文章主要就是参考了TimeGAN和CGAN，可以认为是两者的融合
      - TimeGAN
        - 原始GAN的话，就是两个模型一个Discriminator和一个Generator对抗从而让Generator拟合原始数据分布。Gen的输入是话是随机噪声，输出就是拟合的数据，也就是真实数据的模拟。
        - TimeGAN的主要改动是它又加了两个网络，一个Embedding网络和一个Recovery网络，其实本质就是一个VAE。然后Gen的输入不再是直接的原始数据，而是经过Embedding进行特征提取过后的数据。
      - CGAN
        - 原始GAN的话，就是两个模型一个Discriminator和一个Generator对抗从而让Generator拟合原始数据分布。Gen的输入是话是随机噪声，输出就是拟合的数据，也就是真实数据的模拟。
        - CGAN的话Gen的输入除了随机噪声之外，还有一个Condition Vector，模型输入不同的Condition Vector就能生成不同类型的原始数据的模拟数据。
    - 最开始的想法：训练一个特别大的模型
      - 优化1：时间窗改小，使输入数据量维度变小，模型参数自然变小了。
      - 优化2：分成了多个小模型，训练用GPU，预测时用CPU。也试过用CPU去训练，但是那个训练速度确实有点酸爽，直接放弃了。

- 模型部署

  - 项目启动，在Django配置文件中直接加载训练好的模型，模型是全局的。试过在Rest调用方法里加载，每次会生成一个实例。


  - Rest接口调用，传过来的参数是一段时间序列信息，这个时间序列信息是个二维的，第一个维度是监控指标，第二个维度是监控指标对应的时间序列。

  - Rest接口接受到传来的二维时序数据后，先调用预测模型的predict，输出一个等维度的时间序列，代表预测信息。

  - 当前信息和预测信息都输入到第二个模型也就是故障诊断模型，故障诊断模型输出两个标签，也就是两个数。

  - 将预测模型的输出、故障诊断模型的输出以json的格式返回。前端可以根据这些做可视化。




### TensorFlow分布式训练

- 分布训练的策略
  - 模型并行
    - 指将模型部署到很多设备上运行，比如说多个机器的GPU上。
    - 网络很大，由于显存的限制，很难将模型完整的跑在一个gpu上，因此需要将模型分割成更小的部分，不同部分跑在不同的设备上，例如将网络的不同层运行在不同的设备上。
    - 缺点是各个部分之间存在依赖关系，计算效率不高，只有在模型非常大时才使用这种方式。
  - 数据并行
    - 在多个设备上放置相同的模型，各个设备采用不同的训练样本对模型训练。
    - 一个简单的加速训练的技术是并行地计算梯度，然后更新相应的参数。
    - 具体来说，深度学习模型训练是一个迭代的过程，在每一轮的迭代过程中，前向传播算法根据当前的参数计算出一个batch上数据对应的预测值，然后反向传播算法再根据损失函数计算参数的梯度并更新参数。在分布式训练的过程中，不同设备可以在不同训练数据上运行这个迭代过程，而不同并行模式的区别在于不同的参数更新方式。可以分为同步更新参数和异步更新参数。
    - 异步训练中，各个设备完成一个mini-batch后，就不需要等待其它节点，直接去更新模型的参数。也就是说，在每一轮的迭代中，不同的设备的话它是会去读取参数最新的取值，但是因为不同设备读取参数取值的时间不一样，所以得到的值也是不一样的，所以就有可能产生梯度失效问题。具体来说就是某一时刻某一个设备它读取了模型的参数，然后自己计算梯度，在它计算完之前，另外一个设备将模型的参数更新了，那么这个设备计算过的梯度就过期了，失效了。这个可能会导致模型陷入次优解。这个确实是一个不可避免的问题，但是总体来说异步训练的话模型训练速度是非常快的。
    - 同步训练，就是说所有设备都采用相同的模型参数来训练，等待所有设备的mini-batch上的梯度计算完成后，收集它们的梯度然后进行模型的参数更新。其实就相当于模型在一个更大的batch上训练了，这个更大的batch由每个设备上的mini-batch组成。同步训练的话最好所有设备的计算能力都差不多，否则会对计算资源造成一个浪费的情况。
  - 分布式训练架构
    - Parameter Server架构：最常用的分布式训练架构
      - 集群中的节点分为parameter server和worker。其中parameter server存放模型的参数，而worker负责计算参数的梯度。
      - 在每个迭代过程中，worker从parameter server中获得参数，然后计算的梯度返回给parameter server，parameter server进行梯度聚合，然后更新参数，并将新的参数广播给worker。
      - worker节点数量较多时，ps节点的网络带宽将成为系统的瓶颈。
    - Ring AllReduce
      - 在这种架构下，所有节点都是worker，没有ps来聚合所有worker计算的梯度。Ring AllReduce将设备放置在一个逻辑环路中，每个设备从上行的设备接收数据，并向下行的device发送数据。
      - 首先将每个设备上的梯度tensor切分为长度大致相等的设备数量个分片。
      - ScatterReduce阶段：通过设备数-1轮通信和相加，在每个设备上都计算出一个tensor分片的和。
      - AllGather阶段：通过设备通信，将上个阶段计算出的每个tensor分片的和广播到其他的device
      - 在每个设备上合并分片，得到梯度和，除以设备量，得到平均梯度。



- CNN
  - 卷积操作：卷积核在原始特征图上进行卷积，会得到一个小一些的特征图
  - 感受野：每一层输出的特征图上的一个点对应原始输入图像的个数。感受野越大，集成的信息就越多。
  - 
- RNN
- ResNet
- Transformer
  - Scaled-Dot-Product-Attention
    - Att = （QK^t）V
    - 两个向量做内积，相当于计算余弦值，值越大，代表相似度越高，得到的对应的权重也越高
    - 因为Attention是矩阵乘法，因此很好并行
  - Masked-Multi-head-Attention
    - 计算时，避免在第t时间看到t时间以后的K值，因此需要做一个掩码
    - 具体做法是对于后面的值
